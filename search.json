[
  {
    "objectID": "pages/answer_iris_clustering.html",
    "href": "pages/answer_iris_clustering.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "%matplotlib inline\n\nfrom pandas import DataFrame\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\n\niris = DataFrame(load_iris().data, columns=load_iris().feature_names)\n\nnum_iris_species = len(load_iris().target_names)\n\nkmeans = KMeans(n_clusters=num_iris_species, n_init=\"auto\").fit(iris)\n\na = scatter_matrix(iris, figsize=(16, 16), c=kmeans.labels_)"
  },
  {
    "objectID": "pages/aside_non_linear_regression.html",
    "href": "pages/aside_non_linear_regression.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "Non-linear regression\nSince we’re fitting a straight line, it may seem impossible for this technique to be able to correctly fit more complicated relationships. In general that is true but scikit-learn provides preprocessing tools which can automatically transform your data into something which can be understood linearly.\nLet’s start by making some definitely non-linear data from a sine curve:\nimport numpy as np\nfrom pandas import DataFrame\n\nrng = np.random.RandomState(42)\n\nnumber_of_points = 50\nx_scale = 10\n\nx = x_scale * rng.rand(number_of_points)\ny = np.sin(x) + 0.1 * rng.normal(size=number_of_points)\n\nsin_data = DataFrame({\"x\": x, \"y\": y})\nsin_data.plot.scatter(\"x\", \"y\")\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb3b56fe580&gt;\n\nOf course a linear fit will not work for this data (actually not true, a linear fit will give an answer for this data without trouble, it’s just that the fit will be useless) so let’s allow the data to be transformed by a polynomial basis-function before the linear regression. We put these together into a pipeline using scikit-learns’s make_pipeline() function. This output of this function can be used in the same way as the standard LinearRegression model.\nLet’s make a pipeline which first applies a 7th-order polynomial and then fits it with a linear regression:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_model = make_pipeline(PolynomialFeatures(7), LinearRegression())\nThis poly_model can then be used like the plain LinearRegression model before and have the fit() method called on it.\npoly_model.fit(sin_data[[\"x\"]], sin_data[\"y\"])\n\nxfit = np.linspace(0, x_scale)\nyfit = poly_model.predict(xfit[:, np.newaxis])\n\nax = sin_data.plot.scatter(\"x\", \"y\")\nax.plot(xfit, yfit, linestyle=\":\")\n[&lt;matplotlib.lines.Line2D at 0x7fb3a5bd2f70&gt;]\n\nYou can think of the combined “transform and linear fit” as being a single fit with a 7th-order polynomial.\n\nExercise\n\nChange the order of the polynomial. When does the data start to fit well. What happens when you make the polynomial order very large?"
  },
  {
    "objectID": "pages/answer_blob_inertia.html",
    "href": "pages/answer_blob_inertia.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "%matplotlib inline\n\nfrom pandas import Series, DataFrame\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\ndata, true_labels = make_blobs(n_samples=500, centers=4, random_state=6)\npoints = DataFrame(data, columns=[\"x\", \"y\"])\n\ninertia_values = []\nr = pd.RangeIndex(2, 8)\nfor n_clusters in r:\n    kmeans = KMeans(n_clusters=n_clusters).fit(points)\n    inertia_values.append(kmeans.inertia_)\n\ninertia = Series(inertia_values, name=\"inertia\", index=r)\ninertia.plot()\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f374762f198&gt;"
  },
  {
    "objectID": "pages/answer_find_largest_correlations.html",
    "href": "pages/answer_find_largest_correlations.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "Let’s find the most negative and the most positive (ignoring self-correlation) values\nfrom pandas import DataFrame\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhousing = DataFrame(housing_data.data, columns=housing_data.feature_names)\n\ncorr = housing.corr()\n\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\n1.000000\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\n1.000000\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\n1.000000\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\n1.000000\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\n1.000000\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\n1.000000\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\n1.000000\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\n1.000000\n\n\n\n\n\n\nMost negative correlation\nFind the most negative correlation for each column:\ncorr.min()\nMedInc       -0.119034\nHouseAge     -0.296244\nAveRooms     -0.153277\nAveBedrms    -0.077747\nPopulation   -0.296244\nAveOccup     -0.006181\nLatitude     -0.924664\nLongitude    -0.924664\ndtype: float64\nFind the column which has the lowest correlation:\ncorr.min().idxmin()\n'Latitude'\nExtract the Latitude column and get the index of the most negative value in it:\ncorr[corr.min().idxmin()].idxmin()\n'Longitude'\nThe most negative correlation is therefore between:\ncorr.min().idxmin(), corr[corr.min().idxmin()].idxmin()\n('Latitude', 'Longitude')\nwith the value:\ncorr.min().min()\n-0.9246644339150366\n\n\nMost positive correlation\nFirst we need to remove the 1.0 values on the diagonal:\nimport numpy as np\n\nnp.fill_diagonal(corr.values, np.nan)\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\nNaN\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\nNaN\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\nNaN\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\nNaN\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\nNaN\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\nNaN\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\nNaN\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\nNaN\n\n\n\n\n\ncorr.max().idxmax(), corr[corr.max().idxmax()].idxmax()\n('AveRooms', 'AveBedrms')\ncorr.max().max()\n0.8476213257130424"
  },
  {
    "objectID": "pages/999-contributors.html",
    "href": "pages/999-contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "This course was originally written by Matt Williams, see https://milliams.com/courses/intermediate_python/.\nThe course has since been modified by the Jean Golding Institute.",
    "crumbs": [
      "Contributors"
    ]
  },
  {
    "objectID": "pages/001-workspace-setup.html",
    "href": "pages/001-workspace-setup.html",
    "title": "Workspace setup",
    "section": "",
    "text": "There are lots of different ways to run Python code and many tools to help you write it. You don’t require any special tools to create a Python script, a simple text editor like Notepad on Windows is sufficient. More advanced tools include things like Jupyter Notebooks and IDEs like PyCharm or Visual Studio Code.\nFor this workshop we will be keeping things as simple as possible in order to allow us to focus on the topics we’re learning without having to learn too many extra tools along the way.\nFor the purpose of this course we will be using a free tool called JupyterLab which provides you with a local editor in your web browser where you can write and run Python code. The easiest way to get access to JupyterLab is to install Anaconda which is a piece of software which includes Python along with lots of other tools. It is freely available for Windows, MacOS and Linux.\nAnaconda can be installed into your home area on your computer so if you are on a work laptop, for example, you will not need any special permissions. Once Anaconda is installed, start “Anaconda Navigator” and press the JupyterLab button on the main screen:\n\nThis will open JupyterLab in your default web browser and will look something like this:\n\nThe way that we will be setting up the space is to have a text editor on the left-hand side of the screen and a terminal on the right hand side. We’ll use the editor to write our code and the terminal to run it.\nIn the launcher tab, scoll down to the “Text File” entry and click that. It will turn the editor into a text editor. Then go to File → New and select “Terminal”. It will now have two tabs inside the interface, one labelled “untitled.txt” and the other labelled “Terminal 1”:\n\nThe contents of the Terminal tab will likely be a little different on your computer, compared to what is shown in thise images but that is ok.\nTo make our lives easier, let’s rearange things so that we can see the text editor at the same time as the terminal. Do this by pressing and holding down the left mouse button on the tab that says “Terminal 1” and slowly dragging it to the right-hand side of the window. You’ll see a blue outline like this:\n\nRelease the mouse button and you’ll end up with the two showing side-by-side:\n\n\nWorking directory\nSetting the correct working directory helps organize your project files and ensures that your code can find necessary resources and dependencies. We will revisit this concept later on, but for now be mindful that the space where you save your scripts has to be the same than the working directory in your Command Prompt/Terminal.\nIf you are using the Command Prompt (Windows) you can check your current directory with\n\n\nCommand Prompt\n\ncd\n\nIf you are using a Terminal (MacOS and Linux) you can check your current directory with\n\n\nTerminal\n\npwd\n\nWe’re now ready to get started!",
    "crumbs": [
      "Workspace setup"
    ]
  },
  {
    "objectID": "pages/aside_one_two_dimensional.html",
    "href": "pages/aside_one_two_dimensional.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "scitkit-learn requires the X parameter of the fit() function to be two-dimensional and the y parameter to be one-dimensional.\nX must be two-dimensional, even if there is only one feature (column) present in your data. This can sometimes be a bit confusing as to humans there’s little difference between a table with one column and a simple list of values. Computers, however are very explicit about this difference and so we need to make sure we’re doing the right thing.\nFirst, let’s grab the data we were working with:\nfrom pandas import read_csv\n\ndata = read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/linear.csv\")\n\n2D DataFrames\nIf we look at it, we see it’s a pandas DataFrame which is always inherently two-dimensional:\ndata.head()\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n3.229269\n\n\n\n\n1\n\n\n9.507143\n\n\n14.185654\n\n\n\n\n2\n\n\n7.319939\n\n\n9.524231\n\n\n\n\n3\n\n\n5.986585\n\n\n6.672066\n\n\n\n\n4\n\n\n1.560186\n\n\n-3.358149\n\n\n\n\n\nTo get a more specific idea of the shape of the data structure, we can use the shape attribute:\ndata.shape\n(50, 2)\nThis tell us that it’s a \\((50 \\times 2)\\) structure so is two dimensional.\nTo be explicit, we can also query its dimensionality directly with ndim:\ndata.ndim\n2\n\n\n1D Series\nIf we ask a DataFrame for one of its columns, it returns it to us as a pandas Series. These objects are always one-dimensional (ignoring the potential for multi-indexes):\ndata[\"x\"].head()\n0    3.745401\n1    9.507143\n2    7.319939\n3    5.986585\n4    1.560186\nName: x, dtype: float64\ntype(data[\"x\"])\npandas.core.series.Series\ndata[\"x\"].shape\n(50,)\nNote that the shape is (50,). This might look like it could have multiple values but this is just how Python represents a tuple with one value. To check the dimensionality explicitly, we can peek at ndim again:\ndata[\"x\"].ndim\n1\n\n\n2D subsets of DataFrames\nIf we want to ask a DataFrame for a subset of its columns, it will return the answer to us as a another DataFrame as this is the only way to represent data with multiple columns.\nWe can ask for multiple columns by passing a list of column names to the DataFrame indexing operator.\nPay attention here as the outer pair of square brackets are denoting the indexing operator being called while the inner pair denotes the list being created.\ndata[[\"x\", \"y\"]].head()\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n3.229269\n\n\n\n\n1\n\n\n9.507143\n\n\n14.185654\n\n\n\n\n2\n\n\n7.319939\n\n\n9.524231\n\n\n\n\n3\n\n\n5.986585\n\n\n6.672066\n\n\n\n\n4\n\n\n1.560186\n\n\n-3.358149\n\n\n\n\n\ndata[[\"x\", \"y\"]].shape\n(50, 2)\nWe can see here that when we asked the DataFrame for multiple columns by passing a list of column names it returns a two-dimensional object.\nIf we want to extract just one column but still maintain the dimensionality, we can pass a list with only one column name:\ndata[[\"x\"]].head()\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n\n\n1\n\n\n9.507143\n\n\n\n\n2\n\n\n7.319939\n\n\n\n\n3\n\n\n5.986585\n\n\n\n\n4\n\n\n1.560186\n\n\n\n\n\nIf we check the shape and dimensionality of this, we see that it is a \\((50 \\times 1)\\) structure with two dimensions:\ndata[[\"x\"]].shape\n(50, 1)\ndata[[\"x\"]].ndim\n2\n\n\nFinal comparison\nFinally, to reiterate, the difference between\ndata[\"x\"].head()\n0    3.745401\n1    9.507143\n2    7.319939\n3    5.986585\n4    1.560186\nName: x, dtype: float64\nand\ndata[[\"x\"]].head()\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n\n\n1\n\n\n9.507143\n\n\n\n\n2\n\n\n7.319939\n\n\n\n\n3\n\n\n5.986585\n\n\n\n\n4\n\n\n1.560186\n\n\n\n\n\nis not really in the data itself, but in the mathematical structure. One is a vector and and the other is a matrix. One is one-dimensional and the other is two-dimensional.\ndata[\"x\"].ndim\n1\ndata[[\"x\"]].ndim\n2"
  },
  {
    "objectID": "pages/answer_cluster_bars.html",
    "href": "pages/answer_cluster_bars.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import numpy as np\nfrom pandas import DataFrame, Series\nfrom skimage import io\nfrom sklearn.cluster import KMeans\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\n\npixels_sample = pixels.sample(frac=0.05)\n\nkmeans = KMeans(n_clusters=10, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\n\nlabels = kmeans.predict(pixels[[\"Red\", \"Green\", \"Blue\"]])\n%matplotlib inline\n\nSeries(labels).value_counts(sort=False).plot.bar(color=kmeans.cluster_centers_)\n&lt;Axes: &gt;"
  },
  {
    "objectID": "pages/generate_kmeans_animation.html",
    "href": "pages/generate_kmeans_animation.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "from pandas import DataFrame\nfrom sklearn.datasets import make_blobs\n\nN_CLUSTERS = 3\n\ndata, true_labels = make_blobs(n_samples=500, centers=N_CLUSTERS, random_state=9)\n\npoints = DataFrame(data, columns=[\"x1\", \"x2\"])\npoints.plot.scatter(\"x1\", \"x2\")\n&lt;AxesSubplot:xlabel='x1', ylabel='x2'&gt;\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\nfrom sklearn.cluster import KMeans\n\n# First set up the figure, the axis, and the plot element we want to animate\nwith plt.xkcd():\n    fig, ax = plt.subplots(constrained_layout=True)\n#fig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.tick_params(\n    axis='both',\n    which='both',\n    bottom=False,\n    left=False,\n    labelbottom=False,\n    labelleft=False,\n)\n#ax.set_xlabel(\"x1\")\n#ax.set_ylabel(\"x2\")\n\n# Here we create the initial path objects which will be altered each frame.\np = ax.scatter(points[\"x1\"], points[\"x2\"], edgecolor=\"none\", s=60, alpha=0.8)\nx = ax.scatter([], [], c=\"red\", s=200, marker=\"x\")\n\ndef animate(i):\n    kmeans = KMeans(n_clusters=N_CLUSTERS, n_init=1, max_iter=i+1, random_state=1, init=\"random\").fit(points)\n    p.set_array(kmeans.labels_)  # Update the colours\n    x.set_offsets(kmeans.cluster_centers_)  # Update the positions\n    return p,\n\nanim = animation.FuncAnimation(fig, animate, frames=30, interval=400, blit=True)\n\n#anim.save(\"kmeans.mp4\", extra_args=[\"-vcodec\", \"libx264\"])\nanim.save(\"kmeans.gif\", writer=\"imagemagick\")\n\n!gifsicle -b -O3 kmeans.gif\ngifsicle:kmeans.gif: warning: too many colors, using local colormaps\n  (You may want to try ‘--colors 256’.)"
  },
  {
    "objectID": "pages/appendix_clustering.html",
    "href": "pages/appendix_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a process by which you collect a large number of data points into a smaller number of groups, based on the distances between them. It is useful in cases where the volumes of data are large and you want to extract some figures of interest. It is a type of unsupervised learning.\nA common use for clustering is identifying distinct subsets of a population, e.g. in a census.\nThere are a number of algorithms available for performing clustering but the simplest and most common is k-means clustering.\nIt works by taking the n-dimensional data provided, \\(X\\) and randomly places \\(k\\) seed points in the field which represent the centres of the initial clusters.\n\nIt then iterates over every data point in \\(X\\) and assigns each to be associated with whichever cluster centre is closest.\nOnce all points have been associated with a cluster, it then iterates over each cluster and calculates the new mean of the cluster to be the centroid of all the points assigned to it.\n\nSteps 1 and 2 are repeated until the algorithm converges on a result.\n\n\nA simple example\nLet’s start by using scikit-learn to provide us with some randonly generated data points. It provides a function called make_blobs() which creates a number of gaussian clusters.\nWe’ll ask it to create 500 points in 4 clusters. We set random_state=6 to ensure that this example will always generate the same points for reproducibility.\nfrom sklearn.datasets import make_blobs\n\ndata, true_labels = make_blobs(n_samples=500, centers=4, random_state=6)\nWe then put the data into a pandas DataFrame to give us a nicer API for working with it. We plot it to see what it looks like, colouring each point according to what cluster is was generated from.\n%matplotlib inline\n\nimport pandas as pd\n\npoints = pd.DataFrame(data, columns=[\"x1\", \"x2\"])\npoints.plot.scatter(\"x1\", \"x2\")\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe524046e50&gt;\n\nWe can see here that these clusters are very distinct. This is a very good situation to use k-means clustering in and it will give a useful result.\nWe initialise the KMeans object with the number of clusters we are looking for (n_clusters is a hyperparameter). This is important as k-means requires this decision to be made up-front. There are some clustering algorithms which can attempt to calculate the number of clusters for you but when using k-means you need to make that assessment yourself.\nThere are other hyperparameters that can be passed to KMeans which are explained in full in the documentation.\nPassing the data you want to fit to the fit() method will then actually perform the algormithm. You can pass in nested lists, numpy arrays (as long as they have the shape \\((N_{samples}, N_{features})\\)) or pandas DataFrames.\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, n_init=\"auto\").fit(points)\nNow that we have calculated the cluster centres, we can use the cluster_centers_ data attribute of our model to see what clusters it has decided on.\ncluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=[\"x1\", \"x2\"])\ncluster_centers\n\n\n\n\n\n\n\n\nx1\n\n\nx2\n\n\n\n\n\n\n0\n\n\n6.485156\n\n\n-9.212537\n\n\n\n\n1\n\n\n-7.857994\n\n\n1.892259\n\n\n\n\n2\n\n\n0.485425\n\n\n-1.628580\n\n\n\n\n3\n\n\n7.886559\n\n\n-3.337117\n\n\n\n\n\nComparing these \\(x\\) and \\(y\\) values against the plot above, we see that it seems to have placed the centres in the correct location. It’s better though to be able to see this directly, so let’s plot the centres on top of the original data.\nax = points.plot.scatter(\"x1\", \"x2\")\ncluster_centers.plot.scatter(\"x1\", \"x2\", ax=ax, c=\"red\", s=200, marker=\"x\")\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f75777df2b0&gt;\n\nThe other piece of data that we can retrieve from the model is which cluster it assigned to each data point. This is available as the labels_ data attribute and is an array with 500 entries, each being a number between 0 and 3. We can use it to colour our plot to see the clusters emerge.\npoints.plot.scatter(\"x1\", \"x2\", c=kmeans.labels_, colormap=\"Dark2\", colorbar=False)\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f75773114e0&gt;\n\n\nExercise 1\n\nRun the above example again but try setting different random_state values when creating the blobs. What happens when the generated clusters overlap a lot?\n\n\n\nExercise 2\n\nUsing the data provided by scikit-learn.datasets.load_iris, perform the clustering on the data.\nTip: load the data into a DataFrame with:\nfrom sklearn.datasets import load_iris\niris = pd.DataFrame(load_iris().data, columns=load_iris().feature_names)\nTip: Note that the Iris dataset has four input dimensions. You can pass all four columns into the k-means clustering algorithm with kmeans.fit(iris)\nanswer\n\n\n\nExercise 3 (optional)\n\nAnother data attribute of the model is inertia_ which gives you the sum of the squared distances of data points to their closest cluster center. It is this attribute which the algorithm uses to decide whether it has converged. In general a smaller number represents a better fit.\nWrite a loop which performs the k-means fit over the blob-generated data with the number of clusters varying from 2 up to 7. For each of these fits, extract the value of the inertia_ attribute and draw a plot of inertia against number of clusters.\nanswer\n\n\n\n\nOther clustering algorithms\nThere are different clustering algorithms beyond k-means. scikit-learn come with many and you can see them all in the documentation.\nk-means famously has problems with clusters which are elongated in some direction as it assumes sphericity. Look at the fourth row of this image (where the k-means we are using is the one in the first column):\n\nIt has failed to cluster the three groups in the way that we as humans would have done. Other algorithms (such as DBSCAN) perform better in that situation.\nOne solution with k-means is to transform the data in some way to make it more spherical and then apply clustering. This pre-processing can be done with something like PCA.\nThe other type of preprocessing you may like to do is to manually reparameterise the data. For example, the first row has two circular clusters which share a centre. Since k-means works on cluster-centres it can’t tell them apart. However, if that data was reparameterised into \\((r, \\theta)\\) then the two clusters would become distinct and k-means would work well.\nThis is why it’s always worth plotting your data and deciding how to process it before throwing machine learning algorithms at it."
  },
  {
    "objectID": "pages/500-hyperparameters.html",
    "href": "pages/500-hyperparameters.html",
    "title": "Choosing hyperparameters",
    "section": "",
    "text": "After the exercise in the last chapter, you’re hopefully thinking “why am I spending my time trying different values of n_neighbors, can’t it do this automatically?” If so, you’re in luck!\nIt is a very common thing to need to try out a bunch of different values for your hyperparameters and so scikit-learn provides us with some tools to help out.\nLet’s do a similar thing to the last chapter, but load a different file this time. One with four different classes and follow through the usual steps:\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/blobs.csv\")\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\nimport seaborn as sns\n\nsns.scatterplot(data=data, x=\"x1\", y=\"x2\", hue=\"y\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\nThe tools that allows us to do the hyper-parameter searching is called GridSearchCV which will rerun the model training for every possible hyperparameter that we pass it.\nThe GridSearchCV constructor takes two things: 1. the model that we want to explore, 2. a dictionary containing the hyper-parameter values we want to test.\nIn this case, we are asking it to try every value of n_neighbors from 1 to 49 and it will use the training data to choose the best value.\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nhyperparameters = {\n    \"n_neighbors\" : range(1, 175),\n}\nmodel = GridSearchCV(KNeighborsClassifier(), hyperparameters)\nmodel.fit(train_X, train_y)\n\nGridSearchCV(estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': range(1, 175)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': range(1, 175)}) best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=42)  KNeighborsClassifier?Documentation for KNeighborsClassifierKNeighborsClassifier(n_neighbors=42) \n\n\nThe best way to visualise the data is to plot it. We can do this by grabbing the cv_results_ attribute of GridSearchCV and plotting the mean_test_score against the value of n_neighbors. GridSearchCV will run each experiment multiple times with different splits of training and validation data to provide some measure of uncertainty of the score:\n\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results.plot.scatter(\"param_n_neighbors\", \"mean_test_score\", yerr=\"std_test_score\", figsize=(10,8))\n\n\n\n\n\n\n\n\nOne thing that GridSearchCV does, once it has scanned through all the parameters, is do a final fit using the whole training data set using the best hyperparameters from the search. This allows you to use the GridSearchCV object model as if it were a KNeighborsClassifier object.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(model, X, cmap=\"Pastel2\")\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n\n\n\n\n\n\n\n\nor use it directly with predict:\n\nnew_X = pd.DataFrame({\n    \"x1\": [0, -10, 5, -5],\n    \"x2\": [10, 5, 0, -10],\n})\n\nmodel.predict(new_X)\n\narray([0, 3, 1, 2])\n\n\nor measure its performance against the test data set:\n\nmodel.score(test_X, test_y)\n\n0.936\n\n\nUsing something like GridSearchCV allows you to find the best hyperparameters for your models while keeping them working most generally.\n\n\n\n\n\n\nExercise\n\n\n\nGrab the Iris data set from sklearn. This time we will simplify it down to only two features (sepal length and sepal width) to simplity the visualisation.\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(as_frame=True, return_X_y=True)\nX = X[[\"sepal length (cm)\", \"sepal width (cm)\"]]  # Grab just two of the features\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nUsing GridSearchCV, find the best value of n_neighbors and print the score of that model when run over the test data set.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAs usual, grab the data. The difference this time is that are only going to grab two of the features in order to make it a 2D problem which is easier to visualise.\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX = DataFrame(load_iris().data, columns=load_iris().feature_names)\nX = X[[\"sepal length (cm)\", \"sepal width (cm)\"]]  # Grab just two of the features\ny = load_iris().target\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe will look at values of n_neighbors from 0 to 59.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nhyperparameters = {\n    \"n_neighbors\" : range(1, 40),\n}\nclf = GridSearchCV(KNeighborsClassifier(), hyperparameters).fit(train_X, train_y)\nTo easily look at the results, we put the output into a DataFrame, sort it by the test score (how well that value did against its validation set) and grab the top few rows.\ncv_results = DataFrame(clf.cv_results_)\ncv_results = cv_results.sort_values([\"rank_test_score\", \"mean_test_score\"])\ncv_results.head()[[\"param_n_neighbors\", \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]]\n\n\n\n\n\n\n\n\nparam_n_neighbors\n\n\nmean_test_score\n\n\nstd_test_score\n\n\nrank_test_score\n\n\n\n\n\n\n30\n\n\n31\n\n\n0.795652\n\n\n0.056227\n\n\n1\n\n\n\n\n28\n\n\n29\n\n\n0.795257\n\n\n0.042471\n\n\n2\n\n\n\n\n17\n\n\n18\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n27\n\n\n28\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n29\n\n\n30\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n\nIt looks like the best one is n_neighbors=31 but let’s look on a plot to see how it varies:\ncv_results.plot.scatter(\"param_n_neighbors\", \"mean_test_score\", yerr=\"std_test_score\")\n&lt;Axes: xlabel='param_n_neighbors', ylabel='mean_test_score'&gt;\n\nIndeed n_neighbors=31 is the best in the range but they all have large standard deviations. It’s worth plotting it like this so that you might want to pick a lower mean in order to get a tighter distribution.\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport seaborn as sns\n\nDecisionBoundaryDisplay.from_estimator(clf, X, cmap=\"Pastel2\")\nsns.scatterplot(data=X, x=\"sepal length (cm)\", y=\"sepal width (cm)\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='sepal length (cm)', ylabel='sepal width (cm)'&gt;\n\nclf.score(test_X, test_y)\n0.868421052631579",
    "crumbs": [
      "Choosing hyperparameters"
    ]
  },
  {
    "objectID": "pages/answer_no_y_intercept.html",
    "href": "pages/answer_no_y_intercept.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/linear.csv\")\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=False)\nX = data[[\"x\"]]\ny = data[\"y\"]\nmodel.fit(X, y)\n\n\n\nLinearRegression(fit_intercept=False)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression(fit_intercept=False)\n\n\n\n\n\npred = pd.DataFrame({\"x\": [0, 10]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=data, x=\"x\", y=\"y\")\nsns.lineplot(data=pred, x=\"x\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='x', ylabel='y'&gt;\n\nprint(\" Model gradient: \", model.coef_[0])\nprint(\"Model intercept:\", model.intercept_)\n Model gradient:  1.1985226874421444\nModel intercept: 0.0"
  },
  {
    "objectID": "pages/generate_knn_animation.html",
    "href": "pages/generate_knn_animation.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "from pandas import DataFrame\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\n\nN_CLUSTERS = 2\n\ndata, true_labels = make_blobs(n_samples=500, centers=N_CLUSTERS, random_state=3)\n\npoints = DataFrame(data, columns=[\"x1\", \"x2\"])\nax = sns.scatterplot(data=points, x=\"x1\", y=\"x2\", hue=true_labels, palette=\"Dark2\")\nax.get_legend().set_visible(False)\n\nfrom matplotlib.patches import Circle\nimport numpy as np\n\n#circle_centre = (-2.2, 3.5)\ncircle_centre = (-1.4, 2.6)\npoint_distances = np.sqrt(((points[\"x1\"] - circle_centre[0]).pow(2) + (points[\"x2\"] - circle_centre[1]).pow(2)))\nclosest_point_distances = point_distances.sort_values().iloc[0:5]\nclosest_points = points.loc[closest_point_distances.index]\nclosest_points[\"distance\"] = closest_point_distances\nclosest_points[\"label\"] = true_labels[closest_point_distances.index]\n\nax = sns.scatterplot(data=points, x=\"x1\", y=\"x2\", hue=true_labels, palette=\"Dark2\")\nsns.scatterplot(data=closest_points, x=\"x1\", y=\"x2\", hue=\"label\", palette=\"Dark2\", s=100, alpha=0.5)\nax.get_legend().set_visible(False)\nax.scatter([circle_centre[0]], [circle_centre[1]], s=5)\nax.add_patch(Circle(circle_centre, radius=closest_points[\"distance\"].iloc[-1], facecolor=\"None\", edgecolor=(1, 0.5, 0.5), linewidth=2, alpha=0.8))\nax.axis(\"equal\")\nclosest_points\n\n\n\n\n\n\n\n\nx1\n\n\nx2\n\n\ndistance\n\n\nlabel\n\n\n\n\n\n\n286\n\n\n-0.929114\n\n\n3.003234\n\n\n0.619944\n\n\n0\n\n\n\n\n30\n\n\n-1.014436\n\n\n3.202181\n\n\n0.715040\n\n\n0\n\n\n\n\n75\n\n\n-1.899780\n\n\n3.191116\n\n\n0.774079\n\n\n0\n\n\n\n\n247\n\n\n-2.147328\n\n\n1.727181\n\n\n1.149049\n\n\n1\n\n\n\n\n230\n\n\n-1.660563\n\n\n3.732856\n\n\n1.162435\n\n\n0\n\n\n\n\n\n\nfrom collections import defaultdict\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\nfrom matplotlib import cm\nfrom sklearn.cluster import KMeans\n\n# First set up the figure, the axis, and the plot element we want to animate\nwith plt.xkcd():\n    fig, ax = plt.subplots(constrained_layout=True)\n#fig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.tick_params(\n    axis='both',\n    which='both',\n    bottom=False,\n    left=False,\n    labelbottom=False,\n    labelleft=False,\n)\nax.axis(\"equal\")\n\nax.set_xlim(-5, 2)\nax.set_ylim(0, 5)\n\ncmap = cm.Dark2\n\n# Here we create the initial path objects which will be altered each frame.\np = sns.scatterplot(data=points, x=\"x1\", y=\"x2\", hue=true_labels, palette=\"Dark2\", s=40, alpha=0.8, ax=ax).get_children()[0]\nax.get_legend().set_visible(False)\n#x = ax.scatter([circle_centre[0]], [circle_centre[1]], c=\"red\", s=200, marker=\"x\")\nc = ax.add_patch(Circle(circle_centre, radius=0.0, facecolor=\"None\", edgecolor=\"black\", linewidth=2, alpha=0.6))\n\nFPS = 30\nANIMATION_LENGTH = 6.0  # seconds\n\ntimes_drawn = defaultdict(int)\n\ndef animate(i):\n    max_radius = closest_points.iloc[-1][\"distance\"]*1.01\n    end_pause = 2.0  # seconds\n    max_radius_time = ANIMATION_LENGTH - end_pause  # seconds\n    \n    time=i/FPS\n    new_radius = min(max_radius*(time/max_radius_time), max_radius)  # grow up to max_radius in max_radius_time\n    \n    c.set_radius(new_radius)\n    \n    for index, row in closest_points.iterrows():\n        if new_radius &gt;= row[\"distance\"]:\n            if times_drawn[index] &gt; 0:\n                continue\n            color = cmap.colors[int(round(row[\"label\"]))]\n            ax.scatter([row[\"x1\"]], [row[\"x2\"]], s=160, color=color, alpha=0.5)\n            with plt.xkcd():\n                ax.plot([circle_centre[0], row[\"x1\"]], [circle_centre[1], row[\"x2\"]], color=color, lw=2, alpha=0.6, zorder=-10)\n            times_drawn[index] += 1\n\n    return p,\n\nanim = animation.FuncAnimation(fig, animate, frames=int(FPS*ANIMATION_LENGTH), interval=1000/FPS, blit=True)\n\n#anim.save(\"kmeans.mp4\", extra_args=[\"-vcodec\", \"libx264\"])\nanim.save(\"../img/knn.gif\", writer=\"imagemagick\")\n\n!gifsicle -b -O3 knn.gif\ngifsicle:knn.gif: warning: too many colors, using local colormaps\n  (You may want to try ‘--colors 256’.)"
  },
  {
    "objectID": "pages/appendix_scaling.html",
    "href": "pages/appendix_scaling.html",
    "title": "Feature scaling",
    "section": "",
    "text": "An important part of any data analysis task is preparing your data. Depending on the tak you are trying to do, and the algorithms you are using, the data preparation steps will vary. In this section we will learn about feature scaling and dimensionality reduction.\nLet’s look at some data representing wine. The data contains information on 178 samples of wine, measuring a number of things, including the alcoholic content, the amount of magnesium, the amount of proline and many more. Each sample also has a class associated with it, representing the variety of the grape used.\nfrom sklearn.datasets import load_wine\n\nX, y = load_wine(as_frame=True, return_X_y=True)\ny = y.astype(\"category\")  # This makes seaborn use the right colour palette\nX\n\n\n\n\n\n\n\n\nalcohol\n\n\nmalic_acid\n\n\nash\n\n\nalcalinity_of_ash\n\n\nmagnesium\n\n\ntotal_phenols\n\n\nflavanoids\n\n\nnonflavanoid_phenols\n\n\nproanthocyanins\n\n\ncolor_intensity\n\n\nhue\n\n\nod280/od315_of_diluted_wines\n\n\nproline\n\n\n\n\n\n\n0\n\n\n14.23\n\n\n1.71\n\n\n2.43\n\n\n15.6\n\n\n127.0\n\n\n2.80\n\n\n3.06\n\n\n0.28\n\n\n2.29\n\n\n5.64\n\n\n1.04\n\n\n3.92\n\n\n1065.0\n\n\n\n\n1\n\n\n13.20\n\n\n1.78\n\n\n2.14\n\n\n11.2\n\n\n100.0\n\n\n2.65\n\n\n2.76\n\n\n0.26\n\n\n1.28\n\n\n4.38\n\n\n1.05\n\n\n3.40\n\n\n1050.0\n\n\n\n\n2\n\n\n13.16\n\n\n2.36\n\n\n2.67\n\n\n18.6\n\n\n101.0\n\n\n2.80\n\n\n3.24\n\n\n0.30\n\n\n2.81\n\n\n5.68\n\n\n1.03\n\n\n3.17\n\n\n1185.0\n\n\n\n\n3\n\n\n14.37\n\n\n1.95\n\n\n2.50\n\n\n16.8\n\n\n113.0\n\n\n3.85\n\n\n3.49\n\n\n0.24\n\n\n2.18\n\n\n7.80\n\n\n0.86\n\n\n3.45\n\n\n1480.0\n\n\n\n\n4\n\n\n13.24\n\n\n2.59\n\n\n2.87\n\n\n21.0\n\n\n118.0\n\n\n2.80\n\n\n2.69\n\n\n0.39\n\n\n1.82\n\n\n4.32\n\n\n1.04\n\n\n2.93\n\n\n735.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n173\n\n\n13.71\n\n\n5.65\n\n\n2.45\n\n\n20.5\n\n\n95.0\n\n\n1.68\n\n\n0.61\n\n\n0.52\n\n\n1.06\n\n\n7.70\n\n\n0.64\n\n\n1.74\n\n\n740.0\n\n\n\n\n174\n\n\n13.40\n\n\n3.91\n\n\n2.48\n\n\n23.0\n\n\n102.0\n\n\n1.80\n\n\n0.75\n\n\n0.43\n\n\n1.41\n\n\n7.30\n\n\n0.70\n\n\n1.56\n\n\n750.0\n\n\n\n\n175\n\n\n13.27\n\n\n4.28\n\n\n2.26\n\n\n20.0\n\n\n120.0\n\n\n1.59\n\n\n0.69\n\n\n0.43\n\n\n1.35\n\n\n10.20\n\n\n0.59\n\n\n1.56\n\n\n835.0\n\n\n\n\n176\n\n\n13.17\n\n\n2.59\n\n\n2.37\n\n\n20.0\n\n\n120.0\n\n\n1.65\n\n\n0.68\n\n\n0.53\n\n\n1.46\n\n\n9.30\n\n\n0.60\n\n\n1.62\n\n\n840.0\n\n\n\n\n177\n\n\n14.13\n\n\n4.10\n\n\n2.74\n\n\n24.5\n\n\n96.0\n\n\n2.05\n\n\n0.76\n\n\n0.56\n\n\n1.35\n\n\n9.20\n\n\n0.61\n\n\n1.60\n\n\n560.0\n\n\n\n\n\n178 rows × 13 columns\n\n\nTo keep things easier to visualise, we’ll just grab two of the features:\nX_subset = X[[\"alcohol\", \"proline\"]]\nFirst, let’s have a look at the data and see how it’s distributed:\nimport seaborn as sns\nimport pandas as pd\n\nsns.relplot(data=X, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;seaborn.axisgrid.FacetGrid at 0x7fede99daf90&gt;\n\nThe classes look relatively distinct from each other so a k-nearest neighbours should do the trick. First we split our data in test and train (the _s suffix represents our 2-feature subset):\nfrom sklearn.model_selection import train_test_split\n\ntrain_X_s, test_X_s, train_y_s, test_y_s = train_test_split(X_subset, y, random_state=42)\nThen we fit and score our model, without any further tweaking or tuning:\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndirect_knn = KNeighborsClassifier()\ndirect_knn.fit(train_X_s, train_y_s)\ndirect_knn.score(test_X_s, test_y_s)\n0.6666666666666666\nThat looks like it’s worked, but I would expect a higher score for such a simple data set. Getting a third of the data wrong seems high.\nLet’s use our visualisation function again to see of there’s anything obviously wrong:\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(direct_knn, X_subset, cmap=\"Pastel2\")\nsns.scatterplot(data=X_subset, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='alcohol', ylabel='proline'&gt;\n\nWe can immediately see here that there’s some issue. The kNN algorithm should be creating nice smooth and round regions in the data but they seem here to be be very horizontally striped.\nOne potential reason for this could be that the number of neighbours is incorrect, so you could run a grid search to check the best value for the hyperparameter. In this case however, there’s a more fundamenetal issue with the data.\nIf you look at the absolute values on the \\(x_1\\) and \\(x_2\\) axes you’ll see that the ranges over which they vary are vastly different. The alcohol data goes from 11 to 15, but the proline data goes from 300 to 1700. This causes a problem with algorithms like kNN as the metric that they use to classify is based on euclidean distance. That means that it assumes that a distance of \\(\\Delta x_1\\) has the same importance as a distance \\(\\Delta x_2\\). If we plot our data the way that kNN sees it, it looks like:\nsns.relplot(data=X, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\").set(aspect=\"equal\", xlim=(-800, 800), ylim=(200, 1800))\n&lt;seaborn.axisgrid.FacetGrid at 0x7fede5db1d50&gt;\n\nThere’s such a vast difference in the scales that they look like they’re all in a straight line! It’s clear that the values in the two directions are not equally weighted, so we need to do something about it.\nThe standard way is to take the values in each feature, \\(x\\) and calculate their mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\) and scale the values such that \\(x^{\\prime} = \\frac{x - \\mu}{\\sigma}\\).\nscikit-learn provides a scaler called StandardScaler which can do this for you. It works like any other scikit-learn model and so has a fit() method:\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X)\n\n\n\nStandardScaler()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  StandardScaler?Documentation for StandardScaleriFitted\n\nStandardScaler()\n\n\n\n\n\nAt this point, the scaler has calculated the \\(\\mu\\) and \\(\\sigma\\) for each feature (you can check with scaler.mean_ and scaler.var_). We can ask it to actually perform the scaling by calling the transform() method. This returns a numpy array, so if we want to keep it as as a pandas Dataframe, we need to explicitly convert it back:\nX_scaled_raw = scaler.transform(X)\nX_scaled = pd.DataFrame(X_scaled_raw, columns=X.columns)\nPlotting the result shows a nice balanced spread of data (note the new scales on the axes):\nsns.relplot(data=X_scaled, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;seaborn.axisgrid.FacetGrid at 0x7fede5364690&gt;\n\n\nPipelines\nTo incorporate this scaling process into our model, we need to make sure that: 1. the scaling factors are calculated once and are not changed from that point on, 2. the scaling is applied to the training data, 3. the scaling is applied to the test data, 4. the scaling is applied to any data used in a prediction.\nThe easiest way to ensure this is to use a pipeline. This lets us assemble multiple steps together and scikit-learn knows how to pass data between them.\nfrom sklearn.pipeline import make_pipeline\n\nscaled_knn = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier()\n)\nscaled_knn\n\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  Pipeline?Documentation for PipelineiNot fitted\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\nA pipeline acts like any other scikit-learn model so we can still use our fit, score and predict methods. Any data we pass in will be passed through all the steps in the correct way:\nscaled_knn.fit(train_X_s, train_y_s)\nscaled_knn.score(test_X_s, test_y_s)\n0.8666666666666667\nOur training data has been used to calculate the scaling parameters, has been transformed and then used to fit the kNN model. The test data was then passed through to be scaled and scored using the kNN model.\nWe can plot our decision boundary again and now it’s looking much more reasonable:\nDecisionBoundaryDisplay.from_estimator(scaled_knn, X_subset, cmap=\"Pastel2\")\nsns.scatterplot(data=X_subset, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='alcohol', ylabel='proline'&gt;\n\n\n\nPrincipal component analysis\nIn our data so far, we have used just two of the feature columns in our data. We primarily did this to make plotting the data easier to understand. There’s nothing stopping us from passing all our features to our pipeline:\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)  # re-split using all columns\nscaled_knn_all = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier()\n)\nscaled_knn_all\n\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  Pipeline?Documentation for PipelineiNot fitted\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\nscaled_knn_all.fit(train_X, train_y)\nscaled_knn_all.score(test_X, test_y)\n0.9555555555555556\nSo, adding more features does seem to have improved our score. However, many algorithms (including kNN) are disproportionaly negatively affected by an increase in the number of features. This is due to something called the curse of dimensionality. Our data points become much further apart in n-dimensional space and so the definition of a neighbourhood becomes harder to measure.\nThe solution to this is dimensionality reduction which aims to identify either the most important features, or to find useful combinations of features, which still retain enough of the information in the system.\nThe most common of these is principal component analysis which makes linear combinations of the features into new features. For example, instead of selecting two features (alcohol and proline) to input into the model, we can use PCA to calculate the two most important principal components by combining all the available features.\nThis step can be included in our pipeline, just after the StandardScaler:\nfrom sklearn.decomposition import PCA\n\nscaled_pca_knn = make_pipeline(\n    StandardScaler(),\n    PCA(n_components=2),  # PCA with 2 components\n    KNeighborsClassifier()\n)\nscaled_pca_knn\n\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('pca', PCA(n_components=2)),\n                ('kneighborsclassifier', KNeighborsClassifier())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  Pipeline?Documentation for PipelineiNot fitted\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('pca', PCA(n_components=2)),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n PCA?Documentation for PCA\n\nPCA(n_components=2)\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\nscaled_pca_knn.fit(train_X, train_y)\nscaled_pca_knn.score(test_X, test_y)\n0.9777777777777777\nThat seems to have helped the model.\nIf we want to plot our decision boundary, we need to do a little more work to pull out the transformation steps from the final kNN step. You can access the steps in a pipeline like a Python list:\ntransformer_steps = scaled_pca_knn[:-1]  # all except the last step\nknn_step = scaled_pca_knn[-1]  # only the last step\nso then we can transform the data and pass it, along with the kNN step, into our plotting function:\ntransformed_X = pd.DataFrame(transformer_steps.transform(X))\n\nDecisionBoundaryDisplay.from_estimator(knn_step, transformed_X, cmap=\"Pastel2\")\nsns.scatterplot(data=transformed_X, x=0, y=1, hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='0', ylabel='1'&gt;\n\nThat data is looking nicely separated by class and the decision boundaries are smooth.\n\nNumber of principal components\nIn the example above, I chose n_components=2 so that we’d be able to plot the decision boundary easily. There is however, no reason to limit ourselves to this. Every additional principal component we include increases the amount of information that’s included. We can see how much of the variance in the data is explained by each component:\nscaled_pca_knn[\"pca\"].explained_variance_ratio_\narray([0.3639525 , 0.18617733])\nSo the first provides 36% and the second 19%. It’s more useful to look at the sum of the included components (as you include more components, the number will get closer to 100%):\nsum(scaled_pca_knn[\"pca\"].explained_variance_ratio_)\n0.5501298349276385\nSo only about half the variance was explained by the first two components, but it’s sufficient to have got a very good score previously.\nWe can use the GridSearchCV tool to try different values and see which works best:\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nscaled_pca_knn_cv = GridSearchCV(\n    make_pipeline(\n        StandardScaler(),\n        PCA(),\n        KNeighborsClassifier()\n    ),\n    {\n        \"pca__n_components\" : range(1, 5),\n    }\n)\nscaled_pca_knn_cv\n\n\n\nGridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('pca', PCA()),\n                                       ('kneighborsclassifier',\n                                        KNeighborsClassifier())]),\n             param_grid={'pca__n_components': range(1, 5)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  GridSearchCV?Documentation for GridSearchCViNot fitted\n\nGridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('pca', PCA()),\n                                       ('kneighborsclassifier',\n                                        KNeighborsClassifier())]),\n             param_grid={'pca__n_components': range(1, 5)})\n\n\n\n\n\n\n\n\nestimator: Pipeline\n\nPipeline(steps=[('standardscaler', StandardScaler()), ('pca', PCA()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n PCA?Documentation for PCA\n\nPCA()\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\n\n\n\n\n\nscaled_pca_knn_cv.fit(train_X, train_y)\nscaled_pca_knn_cv.score(test_X, test_y)\n0.9777777777777777\nscaled_pca_knn_cv.best_estimator_[\"pca\"].n_components_\n3\n\n\nExercise\nTake the Iris data set from sklearn and make a classifier which can predict the species. Try building up your solution in the following steps: 1. First use a kNN by itself 2. Add in a feature-scaling step 3. Add in a PCA step\nYou can load the data with\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(as_frame=True, return_X_y=True)"
  },
  {
    "objectID": "pages/300-machine-learning.html",
    "href": "pages/300-machine-learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "As explained at the beginning of the last chapter, machine learning is the name given to the tools, techniques and algorithms which are used to extract information from data.\nThere are two main classes of machine learning:\n\nSupervised This is where you learn about the relationship between some measurement of the data and some label of the data. Once the relationship is established, you can then use it to predict what label to apply to new measurements. Supervised learning falls into two categories:\n\nclassification where the labels are discrete. For example identifying the species of a flower from some measurements of its petals.\nregression where the labels are continuous. For example estimating the price of a house based on its number of rooms, size of garden etc.\n\nUnsupervised This is where you don’t have any label associated with the data and the algorithm will need to extract features of interest itself. Examples of this are:\n\nclustering\ndimensionality reduction\n\n\nIn the last chapter we use a linear regression model. The algorithm that scikit-learn uses is supervised as we were providing it with both the measured quantities (\\(X\\)) and the expected output (or target), \\(y\\). Since the target was a continuous variable we were performing a regression.\nMost of the time when people think of machine learning they are thinking of a supervised algorithm.\n\nThe supervised learning process\n\nThere is a standard process that you go through with most supervised learning problems which is worth understanding as it affects how you should go about data collection as well as the types of problems you can use it to solve. For this explanation, let’s pretend that we want to create a model which can predict the price of a house based on its age, the number of rooms it has and the size of its garden.\nThey all start with the initial data collection. You go out into “the wild” and collect some data, \\(X\\), this could be people’s heights or lengths of leaves of trees or anything in your field which you consider easy to collect/measure. In our example here we would measure a large number of houses’ ages, room counts and garden sizes. We put these data into a table with one row per house and three columns, one for each feature.\nAlongside that you, as an expert, label each data point you collected with some extra data and call that \\(y\\). In our case \\(y\\) is the price of the house. This is something which requires an expert’s eye to assess and we are trying to create a predictive model which can replace the job of the expert house-price surveyor.\nOnce you have collected your data, you need to choose which model best represents the relationship between \\(X\\) and \\(y\\). Perhaps a linear regression is sufficient or maybe you need something more complex. There is no magic solution to knowing which model to use, it comes from experience and experimentation.\nUsing \\(X\\) and \\(y\\) we train (or “fit”) our model to predict the relationship between those two (making sure to split them into train and test subsets to validate our model). After this point the parameters of our model are fixed and can be detached from the data that were used to train it. It is now a “black box” which can make predictions.\nImagine now that a client comes to us and asks us to estimate how much they might be able to sell their house for. They tell us how old the house is, how many rooms it has and the size of its garden. We put these three numbers (\\(x^\\prime\\)) into our model and it returns for us a prediction, \\(\\hat{y}\\).\nOf course, \\(\\hat{y}\\) is just a prediction, it is not necessarily correct. There is usually a true value that we are hoping that we are near to, \\(y\\). We can measure the quality of our model by seeing how close we are to to the true value by subtracting one from the other: \\(y - \\hat{y}\\). This is called the residual.",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "pages/generate_quadratic_plot.html",
    "href": "pages/generate_quadratic_plot.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.xkcd()\n\nN_POINTS = 50\nx = np.linspace(0, 10, N_POINTS)\nrng = np.random.RandomState(42)\ny = x*2\n\nx = x + (rng.normal(size=N_POINTS) * 0.25)\ny = y + (rng.normal(size=N_POINTS) * 0.25)\n\nfig, ax = plt.subplots(constrained_layout=True)\nfig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.scatter(x, y, alpha=0.8)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n\nfig.savefig(\"linear.svg\")\n\ny = -((x-5)**2) + 25\n\nx = x + (rng.normal(size=N_POINTS) * 0.25)\ny = y + (rng.normal(size=N_POINTS) * 0.25)\n\nfig, ax = plt.subplots(constrained_layout=True)\nfig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.scatter(x, y, alpha=0.8)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n\nfig.savefig(\"quadratic.svg\")"
  },
  {
    "objectID": "pages/answer_diabetes_regression.html",
    "href": "pages/answer_diabetes_regression.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "from sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\nX.head()\n\n\n\n\n\n\n\n\nage\n\n\nsex\n\n\nbmi\n\n\nbp\n\n\ns1\n\n\ns2\n\n\ns3\n\n\ns4\n\n\ns5\n\n\ns6\n\n\n\n\n\n\n0\n\n\n0.038076\n\n\n0.050680\n\n\n0.061696\n\n\n0.021872\n\n\n-0.044223\n\n\n-0.034821\n\n\n-0.043401\n\n\n-0.002592\n\n\n0.019907\n\n\n-0.017646\n\n\n\n\n1\n\n\n-0.001882\n\n\n-0.044642\n\n\n-0.051474\n\n\n-0.026328\n\n\n-0.008449\n\n\n-0.019163\n\n\n0.074412\n\n\n-0.039493\n\n\n-0.068332\n\n\n-0.092204\n\n\n\n\n2\n\n\n0.085299\n\n\n0.050680\n\n\n0.044451\n\n\n-0.005670\n\n\n-0.045599\n\n\n-0.034194\n\n\n-0.032356\n\n\n-0.002592\n\n\n0.002861\n\n\n-0.025930\n\n\n\n\n3\n\n\n-0.089063\n\n\n-0.044642\n\n\n-0.011595\n\n\n-0.036656\n\n\n0.012191\n\n\n0.024991\n\n\n-0.036038\n\n\n0.034309\n\n\n0.022688\n\n\n-0.009362\n\n\n\n\n4\n\n\n0.005383\n\n\n-0.044642\n\n\n-0.036385\n\n\n0.021872\n\n\n0.003935\n\n\n0.015596\n\n\n0.008142\n\n\n-0.002592\n\n\n-0.031988\n\n\n-0.046641\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(train_X[[\"bmi\"]], train_y)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\nmodel.score(test_X[[\"bmi\"]], test_y)\n0.3172099449537781\nimport pandas as pd\n\npred = pd.DataFrame({\"bmi\": [X[\"bmi\"].min(), X[\"bmi\"].max()]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=X, x=\"bmi\", y=y)\nsns.lineplot(data=pred, x=\"bmi\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='bmi', ylabel='target'&gt;"
  },
  {
    "objectID": "pages/appendix_clustering_images.html",
    "href": "pages/appendix_clustering_images.html",
    "title": "Clustering images",
    "section": "",
    "text": "As well as abstract data parameters such as created by the make_blobs() function and physical measurements as seen in the iris exercise, clustering can also be used on images.\nThe classic use for this is to reduce the number of colours used in an image for either compression or artistic purposes.\nAs with most machine learning algorithms working on images, the first step is to understand how an image is represented on a computer and to convert that into a format that the algorithm can understand.\nIn it simplest form, an image is represented as a 3-dimensional array of numbers. Two of those dimensions represent the width and height of the image and the third is the colour dimension. The colour dimension usually only has three values in it, one for how green that pixel is, one for how red it is and one for how blue it is. Each of these values will usually be an integer between 0 and 255 (one byte per colour channel).\nThis means that a colour image which is 15 pixels wide and 10 pixels high will have \\(15 \\times 10 \\times 3 = 450\\) numbers used to describe it.\nLet’s start by loading a photo from the internet using scikit-image’s imread() function.\nLooking at the shape we see that it is 480 pixels square and it has 3 colour chanels.\nfrom skimage import io\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/480px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nprint(\"Shape is\", photo.shape)\nprint(\"Size is\", photo.size)\nShape is (480, 480, 3)\nSize is 691200\n%matplotlib inline\nio.imshow(photo)\n&lt;matplotlib.image.AxesImage at 0x7f222bb59810&gt;\n\n(Swallow-tailed bee-eater by Charles J Sharp, CC BY-SA 4.0)\nIt’s often more useful for machine learning to scale the values of the image to be between \\(0\\) and \\(1\\) rather than \\(0\\) and \\(255\\).\nAlso, for the purpose of clustering the colours in an image, we don’t care what positions the pixels have, only their values. To this end, we flatten the (480, 480, 3) 3D array into a 2D array of shape (480 × 480, 3) = (691200, 3)\nimport numpy as np\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\nNow that we have our 2D image array, we put it into a pandas DataFrame for easier plotting and processing:\nfrom pandas import DataFrame\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\n\nExercise\n\nUsing pixels, find the pixel with the highest blue value (tip: use idxmax())\nCan you work out its original (x, y) position in the photo? (tip: use // and %) answer\n\n\n\nExploring the pixel data\nBefore applying the clustering algorithm to the data, it’s useful to plot the data.\nSince RGB pixels are a 3D dataset, we will plot three, 2D plots of the pairs red/green, red/blue and green/blue. To make the plots visually useful we will also colour each point in the plot with the colour of the pixel it came from.\nWe create a new column which contains a label which matplotlib will be able to understand to make each point the correct colour:\nfrom matplotlib import colors\n\npixels[\"colour\"] = [colors.to_hex(p) for p in image_array]\nSince we have \\(480 \\times 480 = 691200\\) pixels, both plotting the data and running the algorithm may be quite slow. To speed things up, we will run the algorithm fit over a random subset of the data. Pandas provides a method for doing just this, sample(). We tell it what fraction of the data we want to look at, here we specify 5%.\npixels_sample = pixels.sample(frac=0.05)\nTo make out lives easier, we define a function plot_colours() which will plot the three pairs of columns against each other\nimport matplotlib.pyplot as plt\n\n\ndef plot_colours(df, c1, c2, c3):\n    \"\"\"\n    Given a DataFrame and three column names,\n    plot the pairs against each other\n    \"\"\"\n    fig, ax = plt.subplots(1, 3)\n    fig.set_size_inches(18, 6)\n    df.plot.scatter(c1, c2, c=df[\"colour\"], alpha=0.3, ax=ax[0])\n    df.plot.scatter(c1, c3, c=df[\"colour\"], alpha=0.3, ax=ax[1])\n    df.plot.scatter(c2, c3, c=df[\"colour\"], alpha=0.3, ax=ax[2])\n\n\nplot_colours(pixels_sample, \"Red\", \"Green\", \"Blue\")\n\nWe see here that there is a strong brown line through the middle caused by the background with the colourful bird feathers are around the edge. In general, k-means clustering struggles to cope well with elongated features like this so we may need a larger number of clusters in order to pick out all the colours we want.\nSometimes viewing the data in 3D can help since planar projections can lose some nuances of the data. Display 3D plots using the mplot3d package.\nfrom mpl_toolkits import mplot3d\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.set_xlabel(\"Red\")\nax.set_ylabel(\"Green\")\nax.set_zlabel(\"Blue\")\nax.scatter(pixels_sample[\"Red\"], pixels_sample[\"Green\"], pixels_sample[\"Blue\"], c=pixels_sample[\"colour\"])\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f222337eb00&gt;\n\nYou can make the 3D plot (and any other in fact) interactive by putting %matplotlib notebook at the top of the cell. This makes the change globally so make sure that you then have another cell which does %matplotlib inline to reset it back to the default static style.\n%matplotlib inline\n\nExercise\n\nCalculate the correlation between then red, green and blue channels. Does it match what you see in the plots? Do you think you want strong or weak correlation between your data variables? answer\n\nNow we get on to the actual work of running the clustering. We do it in the same way as before by first specifying the number of clusters and then passing it the data.\nWe need to specify that the data is pixels_sample[[\"Red\", \"Green\", \"Blue\"]] in order to pick out just those three columns as you should remember that we added in a fourth column containing the encoded colour string.\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=10, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\nFinally, we display the chosen cluster centres which we shall use as our representative colours for the image:\nplt.imshow([kmeans.cluster_centers_])\n&lt;matplotlib.image.AxesImage at 0x7f222055e260&gt;\n\n\n\n\nAssigning points to clusters\nOnce we have calculated the midpoints of each of the clusters, we can then go back through the points in the original data set and assign each point to the cluster centre it is nearest to.\nIn the dummy examples from the previous section we could just use the labels_ data attribute on the model to do this but since we’ve only fit the data over a subset of the full data set, the labels_ attribute will similarly only contain those data points.\nThe KMeans model provides a predict() method which, given the calculated cluster centres can assign a cluster to each data point passed in. This allows you to use k-means clutering as a predictive classification tool.\nlabels = kmeans.predict(pixels[[\"Red\", \"Green\", \"Blue\"]])\nlabels\narray([5, 2, 2, ..., 5, 5, 5], dtype=int32)\n\nExercise (optional)\n\nPlot a bar graph of the counts of the number of pixels in each cluster. Try to colour each bar by the colour of the cluster centre. answer\n\nGiven our list of labels we then loop though it, replacing each cluster index with the values from the corresponding cluster centre. We then reshape the array to make it 3D again (width × height × colour channels).\nreduced = np.array([kmeans.cluster_centers_[p] for p in labels]).reshape(original_shape)\nWe can then plot the reduced image against the original to see the differences.\nf, axarr = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(18, 9))\naxarr[0].imshow(photo)\naxarr[0].set_title(\"Original\")\naxarr[1].imshow(reduced)\naxarr[1].set_title(\"RGB clustered\")\nText(0.5, 1.0, 'RGB clustered')\n\nYou’ll see here that it’s managed to pick out some of the larger blocks of colour but the majority of the image is still brown.\nThere are three main ways we can immediately try to improve this:\n\nIncrease the number of clusters the algorithm is fitting\nTransform the data to reduce the elongated nature of the clusters\nTry a different clustering algorithm (https://scikit-learn.org/stable/modules/clustering.html)\n\nWe’re going to go ahead with method 2 but feel free to play around with changing the number of clusters. Maybe even try plotting the inertia graph like we did in the last section.\n\n\n\nDifferent colour space\nSo far we’ve treated each pixel in the images as being defined by their RGB value, that is their total amount of red, green and blue in each pixel. This is not the only way to describe the colour of a pixel and over the decades, different schemes have emerged. RGB is popular and useful as it is the closes to how a computer monitor or phone screen works where physical red, green and blue lights create the picture.\nThe next most commonly used colour space is probably HSV where the three numbers represent the hue (like on a colour wheel), saturation (the scale from grey to bright) and value (the scale from black to colourful).\nFor the purposes of separating out the visual colour space, there is another colour space called L*a*b* (often just referred to as Lab) which expresses colour as three numerical values, L* for the lightness and a* and b* for the green–red and blue–yellow colour components respectively. Unlike RGB, it is designed to better represent human vision and gives a more uniform representation of brightness.\nDue to how it arranges colours, a plot of a* against b* will often give a useful spread of colours and is probably most likely to show clusters.\nscikit-image provides functions for converting images between different colour spaces, including rgb2lab() and lab2rgb().\nLet’s go through the same steps as before, the only difference being that we start by passing the image though the rgb2lab() function:\nfrom skimage.color import rgb2lab, lab2rgb\n\nphoto_lab = rgb2lab(photo)  # This is where we convert colour space\nw, h, d = original_shape = tuple(photo_lab.shape)\nimage_array_lab = np.reshape(photo_lab, (w * h, d))\n\npixels_lab = DataFrame(image_array_lab, columns=[\"L\", \"a\", \"b\"])\n\npixels_lab[\"colour\"] = [colors.to_hex(p) for p in image_array]\npixels_sample_lab = pixels_lab.sample(frac=0.05)\n\nplot_colours(pixels_sample_lab, \"L\", \"a\", \"b\")\n\nImmediately we see a difference from when we did this plot with RGB. We no longer have the strong diagonal line though the image (though there is still a prominent horizontal line). Looking in the third plot, (a* against b*) we can see distinctly the blue, green and yellow areas.\nPassing this converted data set to KMeans gives us our new set of cluster centres which we can view by converting them back from L*a*b* to RGB.\nkmeans_lab = KMeans(n_clusters=10, n_init=\"auto\").fit(pixels_sample_lab[[\"L\", \"a\", \"b\"]])\nplt.imshow(lab2rgb([kmeans_lab.cluster_centers_]))\n&lt;matplotlib.image.AxesImage at 0x7f220def2d10&gt;\n\nlabels_lab = kmeans_lab.predict(pixels_lab[[\"L\", \"a\", \"b\"]])  # Assign pixels to the cluster centre\ncenters_lab = lab2rgb([kmeans_lab.cluster_centers_])[0]  # Get the RGB of the cluster centres\nreduced_lab = np.array([centers_lab[p] for p in labels_lab]).reshape(original_shape)  # Map and reshape\nio.imshow(reduced_lab)\n&lt;matplotlib.image.AxesImage at 0x7f220dd5d2d0&gt;\n\n\n\nComparing the results\nNow that we have run the analysis with the raw data and the transformed data, we should compare the outcomes.\nf, axarr = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(18, 6))\naxarr[0].imshow(photo)\naxarr[0].set_title(\"Original\")\naxarr[1].imshow(reduced)\naxarr[1].set_title(\"RGB clustered\")\naxarr[2].imshow(reduced_lab)\naxarr[2].set_title(\"Lab clustered\")\nText(0.5, 1.0, 'Lab clustered')\n\nPlotting the three next to each other, we see some differences. The L*a*b* data seems to have picked out more of the different colours of the bird at the expense of some of the brown shades.\nIt is possible that another colour space or a different transform may perform better but under the limit of 10 clusters, it seems that L*a*b* performs a little better.\n\nExercise\n\nExperiment with different numbers of clusters. Is there a point where the two colour space methods become indistinguishable?\nPlot on the same graph the inertia against number of clusters for the two colour spaces. Does one drop faster than the other? (tip: you may need to normalise them against each other) answer\nTry the process with another image. Try to keep the image small (~500 pixels in each dimension) or reduce the frac when sample()ing.\nTry adding in the HSV or HSL colour spaces or any others that scikit-image supports.\n\nThat’s all of the exercises for today. Move on to the next section for some pointers on what you may want to look into next and some book recomendations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This course is aimed at the Python developer who wants to learn how to do useful data analysis tasks. Over the years, Python has become a very popular tool for analysing data. These days it comes with support from many tools to do machine learning, data querying, neural networks and exploratory analysis. In this course we will investigate the use of scikit-learn for machine learning to discover things about whatever data may come across your desk.\nFor the purpose of this course we will be using a free tool called JupyterLab which provides you with a local editor and Python terminal in your web browser. Setting up instructions can be found here.\n\nIntended learning outcomes\nBy the end of this course, you will:\n\nKnow how to use Jupyter Notebooks.\nBe familiar with scikit-learn and seaborn.\nKnow how to perform simple machine learning tasks.\n\n\n\nHow to read this documentation\nIn this documentation, any time that we are seeing a small snippet of Python code, we’ll see it written in a grey box like the following:\nprint(\"Hello, Python\")\nIf the commands are executed by the machine we will see the output of them below enclosed on a vertical purple line:\n\nprint(\"Hello, Python!\")\n\nHello, Python!\n\n\nBy contrast, you will see larger peces of code as scripts with a given name, e.g. script.py, in a code block with darker header:\n\n\nscript.py\n\ngreeting = \"Hello\"\nname = input(\"What is your name? \")\nprint(greeting, name)\n\nWe may ask you to run a script using the Command Prompt (Windows) or Terminal (Mac and Linux). We will show you what commands to run and will look like this:\n\n\nTerminal/Command Prompt\n\npython script.py\n\nPlease note that sometimes we will skip showing the execution of scripts on the Terminal/Command Prompt box, but we will assume you to run the script on your.\nIn some cases we will introduce general programming concepts and structures using pseudocode, a high-level, easy-to-read syntax close to natural language. This should not be confused with Python code and cannot be executed on your machine, but it is useful to describe how your code should behave. Here there is an example:\nFOR EACH sample IN my_study\n    IF (sample.value &gt; 100)\n        DO SOMETHING\n    OTHERWISE\n        DO SOMETHING ELSE\nThere are some exercises along this course, and it is important you try to answer them yourself to understand how Python works. Exercises are shown in blue boxes followed by a yellow box that contains the answer of each exercise. We recommend you to try to answer each exercise yourself before looking at the solution.\n\n\n\n\n\n\nExercise\n\n\n\nThis is an exercise. You will need to click in the below box to see the answer.\n\n\n\n\n\n\n\n\nAnswer (click to open)\n\n\n\n\n\nThis is the answer.\n\n\n\nLast, we will highlight important points using green boxes like this one:\n\n\n\n\n\n\nKey points\n\n\n\nThese are important concepts and technical notes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "pages/answer_knn_moons.html",
    "href": "pages/answer_knn_moons.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "This answer page shows the results of trying different values of noise and n_neighbors when fitting k-NN to a dummy data set. For you to complete the exercise I would just expect you to maually change the values and rerun the cells to look at the differences. On this page I have done something a little more complicated in order to visualise all the combinations in one plot.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nWe’ll loop over a range of neighbour counts:\nneighbours = [1, 5, 10, 100, 150]\nStart by grabbing the data:\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/moons.csv\")\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe then loop over the range of values we want, fit the model and plot the result for each.\n# We'll plot the results in a grid of subplots\nfig, axs = plt.subplots(\n    nrows=len(neighbours),\n    ncols=1,\n    figsize=(8, 30),\n    constrained_layout=True,\n    sharex=True,\n    sharey=True\n)\n\nfor row, n_neighbors in enumerate(neighbours):\n    # Fit and score the model (uses the `n_neighbors` variable)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors).fit(train_X, train_y)\n    score = model.score(test_X, test_y)\n\n    # Plot the results in the grid of subplots\n    ax = axs[row]\n    ax.set_xlim(-2, 3)\n    ax.set_ylim(-1.5, 2)\n    ax.set_title(f\"k: {n_neighbors}, score={score:.2}\")\n    DecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\", ax=ax)\n    sns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, ax=ax, palette=\"Dark2\")\n    ax.get_legend().set_visible(False)"
  },
  {
    "objectID": "pages/400-nearest-neighbours.html",
    "href": "pages/400-nearest-neighbours.html",
    "title": "Nearest Neighbours",
    "section": "",
    "text": "In the first chapters we were using linear regression which is a supervised regression technique. We’re going to carry on with supervised techniques but look instead at how we can classify or categorise data using an algorithm called K-nearest neighbours.\nWhen you ask for a prediction from k-NN for a data point \\(x\\), the algorithm looks at all the training data that was passed in and finds the \\(k\\) nearest (e.g. the 5 nearest) data points. The label of \\(x\\) will then be based on whichever label was found most frequently within the \\(k\\) neighbours.\n\nWe’ll start by grabbing some data which represents a interesting case for some classification methods. Start by loading the data file:\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/moons.csv\")\nIf we look at the first few rows of this data, we can see it has two features (x1 and x2) and one target column (y). The target column is an integer, which suggests it will work well with a classification algorithm:\n\ndata.head()\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0.830858\n-0.334342\n1\n\n\n1\n0.991710\n0.879000\n0\n\n\n2\n1.107245\n-0.470344\n1\n\n\n3\n-0.140899\n1.033148\n0\n\n\n4\n0.405592\n1.328529\n0\n\n\n\n\n\n\n\nLet’s also have a look at the data visually to see what we’re working with:\n\nimport seaborn as sns\n\nsns.scatterplot(data=data, x=\"x1\", y=\"x2\", hue=\"y\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\nWe can grab out the features and target parts now to use in scikit-learn shortly:\n\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\nAs ever, we need to split our data into a training data set and a test data set:\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\nNow we have our data and it’s all in the correct format (\\(X\\) is a 2D table of data and \\(y\\) is a single column of labels), we can go ahead and use our model.\nAs usual it works by importing the model, KNeighborsClassifier, making an instance of it (setting the hyperparameters) and then fitting it by passing it \\(X\\) and \\(y\\) for the training data set.\nThe most important hyperparameter for k-NN is \\(k\\), or the number of neighbours. The model defaults to 5 but you can set it to any integer you wish.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(train_X, train_y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier() \n\n\nAt this point, our model is ready to use. I’ll point out one important difference between k-NN and other algorithms and that is how it stores the information you have given it.\nThinking back to the example of the linear regression from the first chapter, in that case we gave the model some data (50 \\(x\\),\\(y\\) values) and based on those it calculated two parameters of interest, the gradient and the y-intercept. No matter how many training examples we give it, it will always generalise those data down to two parameters.\nK-nearest neighbours is different in that it is a non-generalising learning algorithm (also referred to as instance-based learning). It doesn’t simplify down the training data we pass in, it actually stores all of it internally. Thinking about it, this makes sense as when we ask it to make a prediction it needs to actually go and find the data points that are near the prediction site. This means that if we train a model on more data, the model becomes more heavyweight (i.e. may use more memory) and will likely become slower (as it needs to check more points to find the neighbours).\nIn general, this will not cause a problem but it’s something that you should be aware of.\nWe can use our model in the same way as in the past to, for example, check the performance against the test data set:\n\nmodel.score(test_X, test_y)\n\n0.992\n\n\nThat looks like a very good score indeed. Is that believable or do you think we’ve done something wrong?\nLet’s take a look at the distribution of predictions compared to the input data.\nWe’ll use a built-in function from sklearn called DecisionBoundaryDisplay.from_estimator to plot the predictions of the model, compared to the input data.:\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\")\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n\n\n\n\n\n\n\n\nHere we can see that all of the green points are sitting in the one background area and the orange points are all in their area. In this case, it makes sense that it’s got a very good score since the data are not overlapping much.\n\n\n\n\n\n\nExercise\n\n\n\nRun the code above and make sure you see the same output.\nExperiment with different values of n_neighbors when creating the model (between 1 and 200).\nHow does varying this value affect the prediction map or the model score?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis answer page shows the results of trying different values of noise and n_neighbors when fitting k-NN to a dummy data set. For you to complete the exercise I would just expect you to maually change the values and rerun the cells to look at the differences. On this page I have done something a little more complicated in order to visualise all the combinations in one plot.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nWe’ll loop over a range of neighbour counts:\nneighbours = [1, 5, 10, 100, 150]\nStart by grabbing the data:\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/moons.csv\")\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe then loop over the range of values we want, fit the model and plot the result for each.\n# We'll plot the results in a grid of subplots\nfig, axs = plt.subplots(\n    nrows=len(neighbours),\n    ncols=1,\n    figsize=(8, 30),\n    constrained_layout=True,\n    sharex=True,\n    sharey=True\n)\n\nfor row, n_neighbors in enumerate(neighbours):\n    # Fit and score the model (uses the `n_neighbors` variable)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors).fit(train_X, train_y)\n    score = model.score(test_X, test_y)\n\n    # Plot the results in the grid of subplots\n    ax = axs[row]\n    ax.set_xlim(-2, 3)\n    ax.set_ylim(-1.5, 2)\n    ax.set_title(f\"k: {n_neighbors}, score={score:.2}\")\n    DecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\", ax=ax)\n    sns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, ax=ax, palette=\"Dark2\")\n    ax.get_legend().set_visible(False)",
    "crumbs": [
      "Nearest Neighbours"
    ]
  },
  {
    "objectID": "pages/generate_blobs.html",
    "href": "pages/generate_blobs.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=500, centers=4, cluster_std=2.5, random_state=42)\nX = pd.DataFrame(X, columns=[\"x1\", \"x2\"])\n\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='x1', ylabel='x2'&gt;\n\ndata = X.copy()\ndata[\"y\"] = y\ndata.to_csv(\"blobs.csv\", index=False)\ncheck = pd.read_csv(\"blobs.csv\")\npd.testing.assert_frame_equal(data, check)"
  },
  {
    "objectID": "pages/generate_moons.html",
    "href": "pages/generate_moons.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.2, random_state=42)\nX = pd.DataFrame(X, columns=[\"x1\", \"x2\"])\n\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='x1', ylabel='x2'&gt;\n\ndata = X.copy()\ndata[\"y\"] = y\ndata.to_csv(\"moons.csv\", index=False)\ncheck = pd.read_csv(\"moons.csv\")\npd.testing.assert_frame_equal(data, check)"
  },
  {
    "objectID": "pages/generate_overfit.html",
    "href": "pages/generate_overfit.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import to_rgb, rgb_to_hsv, hsv_to_rgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nplt.xkcd(scale=0.5)\n\np = sns.color_palette(\"colorblind\")\n\nTRUE_COLOR = p[1]\nSAMPLES_COLOR = p[0]#\"steelblue\"\nPOLY_3_COLOR = p[2]#\"darkorange\"\nPOLY_15_COLOR = p[3]#\"darkgreen\"\nTRAIN_COLOR = p[4]#\"crimson\"\nTEST_COLOR = p[5]#\"darkolivegreen\"\n\ndef true_fun(x):\n    return np.cos(1.5 * np.pi * x)\n\nrng = np.random.RandomState(0)\n\nn_samples = 30\n\nx = np.sort(rng.rand(n_samples))\nX = x[:, np.newaxis]\ny = true_fun(x) + rng.randn(n_samples) * 0.1\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=0)\n\ndef poly_fit(X, y, degree):\n    pipeline = make_pipeline(\n        PolynomialFeatures(degree=degree, include_bias=False),\n        LinearRegression(),\n    )\n    pipeline.fit(X, y)\n    return pipeline\n\ndef start_plot():\n    fig, ax = plt.subplots(constrained_layout=True)\n    return fig, ax\n\ndef finalise_plot(ax):\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_xlim((0, 1))\n    ax.set_ylim((-2, 5))\n    ax.legend(loc=\"best\")\n    \ndef plot_model(ax, model, color, label=\"Model\"):\n    X_test = np.linspace(0, 1, 500)[:, np.newaxis]\n    y_pred = model.predict(X_test)\n    ax.plot(X_test, y_pred, label=label, color=color, linestyle=\"--\")\n\ndef plot_true(ax):\n    X_test = np.linspace(0, 1, 100)[:, np.newaxis]\n    ax.plot(X_test, true_fun(X_test), label=\"True function\", color=TRUE_COLOR)\n    \ndef plot_samples(ax, X, y, label, color):\n    edgecolor = hsv_to_rgb(rgb_to_hsv(to_rgb(color)) * [1.0, 0.9, 1.0])\n    ax.scatter(X, y, edgecolor=edgecolor, facecolor=color, s=50, label=label, zorder=100)\n\nTrue function to samples\nimport logging\n# Hide the \"findfont: Font family\" warnings\nlogging.getLogger('matplotlib.font_manager').disabled = True\nfig, ax = start_plot()\nplot_true(ax)\nfinalise_plot(ax)\nfig.savefig(\"overfit_just_model.svg\")\n\nfig, ax = start_plot()\nplot_true(ax)\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nfinalise_plot(ax)\nfig.savefig(\"overfit_model.svg\")\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nfinalise_plot(ax)\nfig.savefig(\"overfit_samples.svg\")\n\n\n\nFitting with different degrees\nmodel = poly_fit(X, y, 3)\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nplot_model(ax, model, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3.svg\")\n\nmodel = poly_fit(X, y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15.svg\")\n\nmodel_3 = poly_fit(X, y, 3)\nmodel_15 = poly_fit(X, y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nplot_model(ax, model_15, POLY_15_COLOR, \"$15^{th}$ degree\")\nplot_model(ax, model_3, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_15.svg\")\n\n\n\nTrain test split\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nfinalise_plot(ax)\nfig.savefig(\"overfit_split.svg\")\n\n\n\n15th order overfits\nmodel = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_train.svg\")\n\nmodel = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_split.svg\")\n\nmodel = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_test.svg\")\n\n\n\n3rd order does not overfit\nmodel = poly_fit(train_X, train_y, 3)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_split.svg\")\n\nmodel = poly_fit(train_X, train_y, 3)\n\nfig, ax = start_plot()\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_test.svg\")\n\n\n\nCompare 3rd and 15th orders directly\nmodel_3 = poly_fit(train_X, train_y, 3)\nmodel_15 = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model_15, POLY_15_COLOR, \"$15^{th}$ degree\")\nplot_model(ax, model_3, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_15_split.svg\")\n\nmodel_3 = poly_fit(train_X, train_y, 3)\nmodel_15 = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model_15, POLY_15_COLOR, \"$15^{th}$ degree\")\nplot_model(ax, model_3, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_15_split.svg\")"
  },
  {
    "objectID": "pages/aside_generate_linear_data.html",
    "href": "pages/aside_generate_linear_data.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nrng = np.random.RandomState(42)\n\nnumber_of_points = 50\nx_scale = 10\ngradient = 2\ny_intercept = -5\n\nx = x_scale * rng.rand(number_of_points)\ny = gradient * x + y_intercept + rng.normal(size=number_of_points)\n\ndata = pd.DataFrame({\"x\": x, \"y\": y})\n\ndata.to_csv(\"linear.csv\", index=False)"
  },
  {
    "objectID": "pages/980-summary.html",
    "href": "pages/980-summary.html",
    "title": "Summary",
    "section": "",
    "text": "That’s all we have for this workshop. By now you should have a better understanding of how you can make your code more easily shared and reusable. In this workshow we have covered:\n\nHow to make you life easier with string formatting\nHow to use dictionaries\nUsing the Python standard library to find and use useful functions\nWays of bundling up your code into reusable units with functions\nMaking it possible to share your code with others by moving code into modules\nHow to produce custom errors\nHow to compactly generate lists with list comprehensions"
  },
  {
    "objectID": "pages/eval and query.html",
    "href": "pages/eval and query.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "In the previous course we\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\ndata = {'city': ['Paris', 'Paris', 'Paris', 'Paris',\n                 'London', 'London', 'London', 'London',\n                 'Rome', 'Rome', 'Rome', 'Rome'],\n        'year': [2001, 2008, 2009, 2010,\n                 2001, 2006, 2011, 2015,\n                 2001, 2006, 2009, 2012],\n        'pop': [2.148, 2.211, 2.234, 2.244,\n                7.322, 7.657, 8.174, 8.615,\n                2.547, 2.627, 2.734, 2.627]}\ndf = DataFrame(data)\ndf\n\n\n\n\n\n\n\n\ncity\n\n\nyear\n\n\npop\n\n\n\n\n\n\n0\n\n\nParis\n\n\n2001\n\n\n2.148\n\n\n\n\n1\n\n\nParis\n\n\n2008\n\n\n2.211\n\n\n\n\n2\n\n\nParis\n\n\n2009\n\n\n2.234\n\n\n\n\n3\n\n\nParis\n\n\n2010\n\n\n2.244\n\n\n\n\n4\n\n\nLondon\n\n\n2001\n\n\n7.322\n\n\n\n\n5\n\n\nLondon\n\n\n2006\n\n\n7.657\n\n\n\n\n6\n\n\nLondon\n\n\n2011\n\n\n8.174\n\n\n\n\n7\n\n\nLondon\n\n\n2015\n\n\n8.615\n\n\n\n\n8\n\n\nRome\n\n\n2001\n\n\n2.547\n\n\n\n\n9\n\n\nRome\n\n\n2006\n\n\n2.627\n\n\n\n\n10\n\n\nRome\n\n\n2009\n\n\n2.734\n\n\n\n\n11\n\n\nRome\n\n\n2012\n\n\n2.627\n\n\n\n\n\ndf.query(\"year == 2001\")\n\n\n\n\n\n\n\n\ncity\n\n\nyear\n\n\npop\n\n\n\n\n\n\n0\n\n\nParis\n\n\n2001\n\n\n2.148\n\n\n\n\n4\n\n\nLondon\n\n\n2001\n\n\n7.322\n\n\n\n\n8\n\n\nRome\n\n\n2001\n\n\n2.547\n\n\n\n\n\ndf[df[\"year\"] == 2001]\n\n\n\n\n\n\n\n\ncity\n\n\nyear\n\n\npop\n\n\n\n\n\n\n0\n\n\nParis\n\n\n2001\n\n\n2.148\n\n\n\n\n4\n\n\nLondon\n\n\n2001\n\n\n7.322\n\n\n\n\n8\n\n\nRome\n\n\n2001\n\n\n2.547\n\n\n\n\n\ndf.query(\"city == 'Paris'\")[\"pop\"].max()\n2.244"
  },
  {
    "objectID": "pages/eval and query.html#asking-questions-of-your-data",
    "href": "pages/eval and query.html#asking-questions-of-your-data",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "In the previous course we\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\ndata = {'city': ['Paris', 'Paris', 'Paris', 'Paris',\n                 'London', 'London', 'London', 'London',\n                 'Rome', 'Rome', 'Rome', 'Rome'],\n        'year': [2001, 2008, 2009, 2010,\n                 2001, 2006, 2011, 2015,\n                 2001, 2006, 2009, 2012],\n        'pop': [2.148, 2.211, 2.234, 2.244,\n                7.322, 7.657, 8.174, 8.615,\n                2.547, 2.627, 2.734, 2.627]}\ndf = DataFrame(data)\ndf\n\n\n\n\n\n\n\n\ncity\n\n\nyear\n\n\npop\n\n\n\n\n\n\n0\n\n\nParis\n\n\n2001\n\n\n2.148\n\n\n\n\n1\n\n\nParis\n\n\n2008\n\n\n2.211\n\n\n\n\n2\n\n\nParis\n\n\n2009\n\n\n2.234\n\n\n\n\n3\n\n\nParis\n\n\n2010\n\n\n2.244\n\n\n\n\n4\n\n\nLondon\n\n\n2001\n\n\n7.322\n\n\n\n\n5\n\n\nLondon\n\n\n2006\n\n\n7.657\n\n\n\n\n6\n\n\nLondon\n\n\n2011\n\n\n8.174\n\n\n\n\n7\n\n\nLondon\n\n\n2015\n\n\n8.615\n\n\n\n\n8\n\n\nRome\n\n\n2001\n\n\n2.547\n\n\n\n\n9\n\n\nRome\n\n\n2006\n\n\n2.627\n\n\n\n\n10\n\n\nRome\n\n\n2009\n\n\n2.734\n\n\n\n\n11\n\n\nRome\n\n\n2012\n\n\n2.627\n\n\n\n\n\ndf.query(\"year == 2001\")\n\n\n\n\n\n\n\n\ncity\n\n\nyear\n\n\npop\n\n\n\n\n\n\n0\n\n\nParis\n\n\n2001\n\n\n2.148\n\n\n\n\n4\n\n\nLondon\n\n\n2001\n\n\n7.322\n\n\n\n\n8\n\n\nRome\n\n\n2001\n\n\n2.547\n\n\n\n\n\ndf[df[\"year\"] == 2001]\n\n\n\n\n\n\n\n\ncity\n\n\nyear\n\n\npop\n\n\n\n\n\n\n0\n\n\nParis\n\n\n2001\n\n\n2.148\n\n\n\n\n4\n\n\nLondon\n\n\n2001\n\n\n7.322\n\n\n\n\n8\n\n\nRome\n\n\n2001\n\n\n2.547\n\n\n\n\n\ndf.query(\"city == 'Paris'\")[\"pop\"].max()\n2.244"
  },
  {
    "objectID": "pages/600-correlation.html",
    "href": "pages/600-correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "When presented with a new collection of data, one of the first questions you may ask is how they are related to each other. This can involve deep study of how one parameter is likely to vary as you change another but the simplest start is to look a the linear correlation between them.\nCorrelation is usually taught as being the degree to which two variables are linearly related, that is as one increases, on average how much does the other one increase. This is a useful measure because it’s easy to calculate and most data only have either linear relationships or no relationship at all.\n\nHowever, correlation is a much broader idea than that and when doing machine learning, it’s worth understanding the bigger picture. At its core, correlation is a measure of how related two data sets are. The way I like to think of it is, if I know the value of one of the two ariables, how much information do I have about the value of the other.\nTo highlight this, consider the following two variables, \\(x\\) and \\(y\\):\n\nThey have a linear correlation of zero (on average as \\(x\\) increases, \\(y\\) stays the same) but if you know the value of \\(X\\), you clearly have information about what the value of \\(y\\) is likely to be.\nThe other way to think about it is in terms of mutual information. \\(y\\) is clearly sharing information with \\(x\\), otherwise there would be no visible pattern.\n\nMultiple cross-correlation\nIt’s very common when working on real data that you have more than two figures of interest.\nTo get a sense of some real data, let’s look at a housing dataset provided by scikit-learn.\n\nfrom sklearn.datasets import fetch_california_housing\n\nhousing, target = fetch_california_housing(as_frame=True, return_X_y=True)\n\n\nhousing.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\nIt has a row for each census block and a column for each feature, e.g. “median income of the block”, “average house age of the block” etc.\nTo get the linear correlation between all these features, we call the corr() method on the DataFrame:\n\nhousing.corr()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nMedInc\n1.000000\n-0.119034\n0.326895\n-0.062040\n0.004834\n0.018766\n-0.079809\n-0.015176\n\n\nHouseAge\n-0.119034\n1.000000\n-0.153277\n-0.077747\n-0.296244\n0.013191\n0.011173\n-0.108197\n\n\nAveRooms\n0.326895\n-0.153277\n1.000000\n0.847621\n-0.072213\n-0.004852\n0.106389\n-0.027540\n\n\nAveBedrms\n-0.062040\n-0.077747\n0.847621\n1.000000\n-0.066197\n-0.006181\n0.069721\n0.013344\n\n\nPopulation\n0.004834\n-0.296244\n-0.072213\n-0.066197\n1.000000\n0.069863\n-0.108785\n0.099773\n\n\nAveOccup\n0.018766\n0.013191\n-0.004852\n-0.006181\n0.069863\n1.000000\n0.002366\n0.002476\n\n\nLatitude\n-0.079809\n0.011173\n0.106389\n0.069721\n-0.108785\n0.002366\n1.000000\n-0.924664\n\n\nLongitude\n-0.015176\n-0.108197\n-0.027540\n0.013344\n0.099773\n0.002476\n-0.924664\n1.000000\n\n\n\n\n\n\n\nHere we see the features in our data set along both the rows and the columns. The correlation between each pair is given as a number between -1.0 and 1.0 where -1.0 is absolute inverse linear correlation, 1.0 is absolute positive linear correlation and zero is no linear correlation.\nWe see the the 1.0 occuring on the diagonal (because a variable is always completely correlated with itself) and a whole range of values between -1.0 and 1.0 off-diagonal.\nIf we want the correlation between two specific columns then we can request it from this object:\n\ncorr = housing.corr()\ncorr[\"MedInc\"][\"AveRooms\"]\n\nnp.float64(0.3268954316412978)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLook through the table manually and see if you can find the most negative and most positive correlations.\nBonus: Try to automate that search using Python code. - Hint: To find the minimum, use the min() and idxmin() methods. To find the maximum, hide the diagonals first using np.fill_diagonal(corr.values, np.nan)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLet’s find the most negative and the most positive (ignoring self-correlation) values\nfrom pandas import DataFrame\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhousing = DataFrame(housing_data.data, columns=housing_data.feature_names)\n\ncorr = housing.corr()\n\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\n1.000000\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\n1.000000\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\n1.000000\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\n1.000000\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\n1.000000\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\n1.000000\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\n1.000000\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\n1.000000\n\n\n\n\n\n\nMost negative correlation\nFind the most negative correlation for each column:\ncorr.min()\nMedInc       -0.119034\nHouseAge     -0.296244\nAveRooms     -0.153277\nAveBedrms    -0.077747\nPopulation   -0.296244\nAveOccup     -0.006181\nLatitude     -0.924664\nLongitude    -0.924664\ndtype: float64\nFind the column which has the lowest correlation:\ncorr.min().idxmin()\n'Latitude'\nExtract the Latitude column and get the index of the most negative value in it:\ncorr[corr.min().idxmin()].idxmin()\n'Longitude'\nThe most negative correlation is therefore between:\ncorr.min().idxmin(), corr[corr.min().idxmin()].idxmin()\n('Latitude', 'Longitude')\nwith the value:\ncorr.min().min()\n-0.9246644339150366\n\n\nMost positive correlation\nFirst we need to remove the 1.0 values on the diagonal:\nimport numpy as np\n\nnp.fill_diagonal(corr.values, np.nan)\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\nNaN\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\nNaN\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\nNaN\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\nNaN\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\nNaN\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\nNaN\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\nNaN\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\nNaN\n\n\n\n\n\ncorr.max().idxmax(), corr[corr.max().idxmax()].idxmax()\n('AveRooms', 'AveBedrms')\ncorr.max().max()\n0.8476213257130424\n\n\n\n\n\n\nPlotting the correlation\nViewing the correlation coefficients as a table is useful if you want the precise value of the correlation but often you want a visual overview which can give you the information you want at a glance.\nThe easiest way to view it is as a heat map where each cell has a colour showing the value of the correlation using Seaborn which is a visualisation library that provides a higher-level interface to Matplotlib.\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, square=True, cmap=\"RdBu\")\n\n\n\n\n\n\n\n\nThis gives us a sense of which parameters are strongly correlated with each other. Very blue squares are positively correlated, for example the average number of rooms and the average number of bedrooms. That correlation makes sense as they definitely have mutual information.\nOthers perhaps make less sense at a glance. We see that the latitude is very strongly negatively correlated with the longitude. Why on earth should there be any relationship between those two? Let’s take a look at another view on the data to see if we can discover why.\n\n\nMulti-variable scatter matrix\nPandas also provides a quick method of looking at a large number of data parameters at once and looking visually at which might be worth investigating. If you pass any pandas DataFrame to the scatter_matrix() function then it will plot all the pairs of parameters in the data.\nThe produced graph has a lot of information in it so it’s worth taking some time to make sure you understand these plots. The plot is arranged with all the variables of interest from top to bottom and then repeated from left to right so that any one square in the grid is defined by the intersection of two variables.\nEach box that is an intersection of a variable with another (e.g. row three, column one is the intersection between “AveRooms” and “MedInc”) shows the scatter plot of how the values of those variables relate to each other. If you see a strong diagonal line it means that those variables are correlated in this data set. It it’s more of a blob or a flat horizontal or vertical line then that suggests a low correlation.\nThe top-right triangle of the plot is a repeat of the bottom-left triangle, just with the items in the pair reversed (i.e. row one, column three is the intersection between “MedInc” and “AveRooms”).\nThe square boxes along the diagonal from the top-left to the bottom-right are those intersections of a variable with itself and so are used, not to show correlation, but to show the distribution of values of each single variable as a histogram.\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(housing, figsize=(16, 16))\n\n\n\n\n\n\n\n\nIn general, when calculating a regression, you want your features to be as uncorrelated with each other as possible. This is because if two features, \\(x_1\\) and \\(x_2\\) are strongly correlated with each other then it’s possible to predict the value of \\(x_2\\) from the value of \\(x_1\\) with high confidence. This means that \\(x_2\\) is not providing any additional predictive power.\nIn some cases this is not a problem as adding one extra variable does not slow down or harm the algorithm used but some methods benefit from choosing carefully the parameters which are being fitted over.\nIt’s also possible in some cases to transform the data in some way to reduce the correlation between variables. One example of a method which does this is principle component analysis (PCA).\nOn the other hand, you do want correlation between \\(X\\) and \\(y\\) as if there is no mutual information then there is no predictive power.\n\n\n\n\n\n\nExercise\n\n\n\nTry running through the above step using a different dataset from sklearn. You can find them listed at https://scikit-learn.org/stable/datasets/toy_dataset.html. Iris is a classic dataset used in machine learning which it is worth being aware of.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\n\niris, iris_target = load_iris(as_frame=True, return_X_y=True)\niris.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\ncorr = iris.corr()\n%matplotlib inline\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, cmap=\"RdBu\")\n&lt;AxesSubplot: &gt;\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(iris, figsize=(16, 16), c=iris_target)",
    "crumbs": [
      "Correlation"
    ]
  },
  {
    "objectID": "pages/answer_colour_space_inertia.html",
    "href": "pages/answer_colour_space_inertia.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "First we get the RGB data set:\nimport numpy as np\nfrom pandas import Series, DataFrame\nimport pandas as pd\nfrom skimage import io\nfrom sklearn.cluster import KMeans\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\n\npixels_sample = pixels.sample(frac=0.05)\nThen we compute the L*a*b* dataset:\nfrom skimage.color import rgb2lab\n\nphoto_lab = rgb2lab(photo)  # This is where we convert colour space\nw, h, d = original_shape = tuple(photo_lab.shape)\nimage_array_lab = np.reshape(photo_lab, (w * h, d))\n\npixels_lab = DataFrame(image_array_lab, columns=[\"L\", \"a\", \"b\"])\n\npixels_sample_lab = pixels_lab.sample(frac=0.05)\nThen we normalise the two inertia values so we can compare them alongside each other:\nkmeans = KMeans(n_clusters=1, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\nkmeans_lab = KMeans(n_clusters=1, n_init=\"auto\").fit(pixels_sample_lab[[\"L\", \"a\", \"b\"]])\n\nnorm = kmeans_lab.inertia_ / kmeans.inertia_\nThen we loop over the number of clusters and calculate the inertia of each:\n%matplotlib inline\n\ninertia_values = []\nr = pd.RangeIndex(2, 10)\nfor n_clusters in r:\n    kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\n    kmeans_lab = KMeans(n_clusters=n_clusters, n_init=\"auto\").fit(pixels_sample_lab[[\"L\", \"a\", \"b\"]])\n    inertia_values.append((kmeans.inertia_, kmeans_lab.inertia_ / norm))\n\ninertia = DataFrame(inertia_values, columns=[\"RGB\", \"L*a*b*\"], index=r)\ninertia.plot()\n&lt;Axes: &gt;"
  },
  {
    "objectID": "pages/200-validation.html",
    "href": "pages/200-validation.html",
    "title": "Testing your model",
    "section": "",
    "text": "So far we have been fitting our model to the data and assuming that it is doing a good job. We have not had any method for analysing the quality of the fit. In our example so far we first looked at the fit with our eyes and judged it sufficient. We then compared the output parameters with our ground-truth and judged it to be “close enough”.\nTo truly judge how good the model is, we need to compare it with some data and see how well it aligns (i.e. how well it would be able to predict it).\nNaïvely we might think to compare our model against the same data we used to fit it. However, this is a dangerous thing to do as it encourages you to tweak your model to best fit the data that you have in hand rather than trying to make a model which can predict things about the process which generated your data. Making your model fit your local subset well, at the expense of the global superset is known as overfitting.\nFor example, imagine we have a true physical model:\n\nIf we want to understand the underlying model, we can make measurements of it:\n\nHowever, we cannot see the underlying model directly so all that we see is:\n\nWe can fit this model with perhaps varying degrees of polynomial. Mathematically, if we increase the degree of polynomial far enough we can fit any function. For example, fitting the data with a 15th-order polynomial creates a model which goes through most of the data points but clearly represent the underlying model badly:\n\nHere we can see that the model is not doing a good job of representing the underlying function (because we saw it above) but in the real world you do not usually have the underlying model available. In those cases overfitting is harder to see as it just manifests as a “well-performing” model. Seen in isolation, this model looks like it is performing quite well, whereas a 3rd-order polynomial looks slightly worse (as fewer of the bumps are accounted for):\n\nThe simplest solution to overfitting is to fit your model with one subset of data and then assess its quality with another subset. If those two subsets are independent then any specific features in the former which your model might try to overfit to will not be present in the latter and so it will be judged poorly.\n\nBringing up our data from the last chapter again:\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/linear.csv\")\n\nX = data[[\"x\"]]\ny = data[\"y\"]\nscikit-learn provides a built-in function, train_test_split, to split your data into a subset of data to fit with and a subset of data to test against:\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\n\ntrain_test_split will split your data randomly and so to get a reproducible “random” split we set the random_state argument.\nTo see that train and test are taken from the same distribution let’s plot them:\n\nimport seaborn as sns\n\n# Label the original DataFrame with the test/train split\n# This is just used for plotting purposes\ndata.loc[train_X.index, \"train/test\"] = \"train\"\ndata.loc[test_X.index, \"train/test\"] = \"test\"\n\nsns.relplot(data=data, x=\"x\", y=\"y\", hue=\"train/test\")\n\n\n\n\n\n\n\n\nNow that we have train and test we should only ever pass train to the fit function:\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(train_X, train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nTo find out how good the fit was, we can call the score method on the model. It is important here that we pass in our test data set as we expect that to provide an independent validation of the model.\n\nmodel.score(test_X, test_y)\n\n0.9676069631786153\n\n\nA score of \\(1.0\\) is a perfect match and anything less than that is less-well performing. A score of \\(0.97\\) suggests we have a very good model.\nGoing back to our example from the start, we can see that when we compare our 3rd- and 15th-order polynomials against the test data, the 3rd-order score will be much better:\n\n\n\n\n\n\n\nExercise\n\n\n\n\nLoad the diabetes dataset from the scikit-learn collection. You can load the dataset into pandas with:\nfrom sklearn.datasets import load_diabetes\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\nSplit the data into train and test subsets\nFit and plot the linear relationship between the “bmi” column (Body mass index) and the “target” column (quantitative measure of disease progression one year after baseline).\nCalculate the score of the model against the test data set.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\nX.head()\n\n\n\n\n\n\n\n\nage\n\n\nsex\n\n\nbmi\n\n\nbp\n\n\ns1\n\n\ns2\n\n\ns3\n\n\ns4\n\n\ns5\n\n\ns6\n\n\n\n\n\n\n0\n\n\n0.038076\n\n\n0.050680\n\n\n0.061696\n\n\n0.021872\n\n\n-0.044223\n\n\n-0.034821\n\n\n-0.043401\n\n\n-0.002592\n\n\n0.019907\n\n\n-0.017646\n\n\n\n\n1\n\n\n-0.001882\n\n\n-0.044642\n\n\n-0.051474\n\n\n-0.026328\n\n\n-0.008449\n\n\n-0.019163\n\n\n0.074412\n\n\n-0.039493\n\n\n-0.068332\n\n\n-0.092204\n\n\n\n\n2\n\n\n0.085299\n\n\n0.050680\n\n\n0.044451\n\n\n-0.005670\n\n\n-0.045599\n\n\n-0.034194\n\n\n-0.032356\n\n\n-0.002592\n\n\n0.002861\n\n\n-0.025930\n\n\n\n\n3\n\n\n-0.089063\n\n\n-0.044642\n\n\n-0.011595\n\n\n-0.036656\n\n\n0.012191\n\n\n0.024991\n\n\n-0.036038\n\n\n0.034309\n\n\n0.022688\n\n\n-0.009362\n\n\n\n\n4\n\n\n0.005383\n\n\n-0.044642\n\n\n-0.036385\n\n\n0.021872\n\n\n0.003935\n\n\n0.015596\n\n\n0.008142\n\n\n-0.002592\n\n\n-0.031988\n\n\n-0.046641\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(train_X[[\"bmi\"]], train_y)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\nmodel.score(test_X[[\"bmi\"]], test_y)\n0.3172099449537781\nimport pandas as pd\n\npred = pd.DataFrame({\"bmi\": [X[\"bmi\"].min(), X[\"bmi\"].max()]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=X, x=\"bmi\", y=y)\nsns.lineplot(data=pred, x=\"bmi\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='bmi', ylabel='target'&gt;",
    "crumbs": [
      "Testing your model"
    ]
  },
  {
    "objectID": "pages/answer_iris_correlation.html",
    "href": "pages/answer_iris_correlation.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "from pandas import DataFrame\nfrom sklearn.datasets import load_iris\n\niris, iris_target = load_iris(as_frame=True, return_X_y=True)\niris.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\ncorr = iris.corr()\n%matplotlib inline\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, cmap=\"RdBu\")\n&lt;AxesSubplot: &gt;\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(iris, figsize=(16, 16), c=iris_target)"
  },
  {
    "objectID": "pages/answer_find_bluest_pixel.html",
    "href": "pages/answer_find_bluest_pixel.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import numpy as np\nfrom pandas import DataFrame, Series\nfrom skimage import io\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\nTo find the index of the pixel with the largest blue value, we use idxmax() on the Blue column.\nbluest_index = pixels[\"Blue\"].idxmax()\nWe use the width, w, of the original image as the denominator for a division and a modulo.\nx = bluest_index % w\ny = bluest_index // w\nx, y\n(537, 150)"
  },
  {
    "objectID": "pages/990-what-next.html",
    "href": "pages/990-what-next.html",
    "title": "What next",
    "section": "",
    "text": "Further topics\nHere are some additional chapters to work through on a number of different topics. Choose the ones that interest you.\nFeature scaling and principal component analysis are important parts of many data analysis pipelines.\nClustering is an unsupervised classification method.\nImage Clustering uses clustering techniques to demonstrate a form of image compression.\n\n\nOther concepts\nThis course has provided a quick overview of some of the basic data analysis and machine learning tools available. Of course it could not cover the full breadth of possible topics so here I will give some pointers to things you may want to learn next.\nNaïve bayes classification is a supervised classification method which works by attempting to a model by assuming that the data you present it with was created from that model originally. It uses Bayesian statistics to work out which model parameters best describe the distribution of data. Read more at scikit-learn.\nSupport vector machines is another supervised classification method which tries to find the dividing line between different classes. Read more at scikit-learn.\nDecision trees is a supervised classification method which creates a tree of binary choices in order to assign a class to a data point. For example, on a population data set, the first question might be “is the person’s height above 1.6 m”. Depending on the answer to that, the next question asked may be different. The path through the tree depends on the exact details of the data point and so each leaf will be associated with a predicted class. Due to the large number of potential parameter combinations, DTs require more data that many other methods but are capable of creating a more nuanced response. Read more at scikit-learn.\nNeural networks are the most widely know technique and are generally used as a classification tool for both supervised and unsupervised situations. They are very versatile and are often the first tool reached for by data scientists, even when there is a simpler method available.\nK-Folds cross-validation is a more advanced technique for testing and validating your models. It greatly increases the time to fit your model but if you can afford it it is worth using. scikit-learn has built-in suport for it.\n\n\nFurther reading\n\nPython Data Science Handbook by Jake VanderPlas\nscikit-learn documentation\nHands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition by Aurélien Géron.",
    "crumbs": [
      "What next"
    ]
  },
  {
    "objectID": "pages/answer_pixel_correlation.html",
    "href": "pages/answer_pixel_correlation.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import numpy as np\nfrom pandas import DataFrame, Series\nfrom skimage import io\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\npixels.corr()\n\n\n\n\n\n\n\n\nRed\n\n\nGreen\n\n\nBlue\n\n\n\n\n\n\nRed\n\n\n1.000000\n\n\n0.85383\n\n\n0.749801\n\n\n\n\nGreen\n\n\n0.853830\n\n\n1.00000\n\n\n0.781750\n\n\n\n\nBlue\n\n\n0.749801\n\n\n0.78175\n\n\n1.000000"
  },
  {
    "objectID": "pages/answer_negative_correlation.html",
    "href": "pages/answer_negative_correlation.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "import numpy as np\nfrom pandas import Series, DataFrame\n\na = np.arange(100)\nb = np.arange(100) * -2\ndf = DataFrame({\"a\": a, \"b\": b})\n\ndf.corr()\n\n\n\n\n\n\n\n\na\n\n\nb\n\n\n\n\n\n\na\n\n\n1.0\n\n\n-1.0\n\n\n\n\nb\n\n\n-1.0\n\n\n1.0"
  },
  {
    "objectID": "pages/answer_gridsearch_knn_iris.html",
    "href": "pages/answer_gridsearch_knn_iris.html",
    "title": "Applied Data Analysis in Python",
    "section": "",
    "text": "As usual, grab the data. The difference this time is that are only going to grab two of the features in order to make it a 2D problem which is easier to visualise.\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX = DataFrame(load_iris().data, columns=load_iris().feature_names)\nX = X[[\"sepal length (cm)\", \"sepal width (cm)\"]]  # Grab just two of the features\ny = load_iris().target\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe will look at values of n_neighbors from 0 to 59.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nhyperparameters = {\n    \"n_neighbors\" : range(1, 40),\n}\nclf = GridSearchCV(KNeighborsClassifier(), hyperparameters).fit(train_X, train_y)\nTo easily look at the results, we put the output into a DataFrame, sort it by the test score (how well that value did against its validation set) and grab the top few rows.\ncv_results = DataFrame(clf.cv_results_)\ncv_results = cv_results.sort_values([\"rank_test_score\", \"mean_test_score\"])\ncv_results.head()[[\"param_n_neighbors\", \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]]\n\n\n\n\n\n\n\n\nparam_n_neighbors\n\n\nmean_test_score\n\n\nstd_test_score\n\n\nrank_test_score\n\n\n\n\n\n\n30\n\n\n31\n\n\n0.795652\n\n\n0.056227\n\n\n1\n\n\n\n\n28\n\n\n29\n\n\n0.795257\n\n\n0.042471\n\n\n2\n\n\n\n\n17\n\n\n18\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n27\n\n\n28\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n29\n\n\n30\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n\nIt looks like the best one is n_neighbors=31 but let’s look on a plot to see how it varies:\ncv_results.plot.scatter(\"param_n_neighbors\", \"mean_test_score\", yerr=\"std_test_score\")\n&lt;Axes: xlabel='param_n_neighbors', ylabel='mean_test_score'&gt;\n\nIndeed n_neighbors=31 is the best in the range but they all have large standard deviations. It’s worth plotting it like this so that you might want to pick a lower mean in order to get a tighter distribution.\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport seaborn as sns\n\nDecisionBoundaryDisplay.from_estimator(clf, X, cmap=\"Pastel2\")\nsns.scatterplot(data=X, x=\"sepal length (cm)\", y=\"sepal width (cm)\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='sepal length (cm)', ylabel='sepal width (cm)'&gt;\n\nclf.score(test_X, test_y)\n0.868421052631579"
  },
  {
    "objectID": "pages/100-fitting-data.html",
    "href": "pages/100-fitting-data.html",
    "title": "Fitting data",
    "section": "",
    "text": "The process of extracting information from data using computers is called machine learning.\nMachine learning a very large field and covers a whole host of techniques. In this course we will be discovering a few of them but let’s first start with the simplest form of machine learning, the linear fit or linear regression.\n\nInput data\nFirst we shall need some data. For this example we shall be pretending that we have measured these data from an experiment and we want to extract the underlying parameters of the system that generated them.\nI have prepared a CSV file which you can read into pandas using read_csv:\n\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/linear.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n3.745401\n3.229269\n\n\n1\n9.507143\n14.185654\n\n\n2\n7.319939\n9.524231\n\n\n3\n5.986585\n6.672066\n\n\n4\n1.560186\n-3.358149\n\n\n\n\n\n\n\nWe can see here that the data has two columns, x and y. Traditionally, \\(x\\) is used for the things we can easily measure in the world and use as inputs, and \\(y\\) is used for the thing we want to predict. In our case, we want to work out what the value of \\(y\\) should be for any given \\(x\\).\nIn this case we have one single \\(x\\) column but in a more complicated data set we may have multiple \\(x_1\\), \\(x_2\\) etc. in which case the set together is sometimes given the upper-case letter \\(X\\). Each \\(x_n\\) column is called a feature. Features are usually things that you have measured as part of a experiment (e.g. height of a person, temperature of a room, size of a garden etc.).\nOur \\(y\\) column is the thing that we are going to create a model to predict the value of. The \\(y\\) column of the input data is often called the label or the target.\nLet’s check how many rows we have:\n\ndata.count()\n\nx    50\ny    50\ndtype: int64\n\n\nWe have 50 rows here. In the input data, each row is often called a sample (though sometimes also called an instance, example or observation). For example, it could be the information about a single person from a census or the measurements at a particular time from a weather station.\nLet’s have a look at what the data looks like when plotted:\n\nimport seaborn as sns\n\nsns.relplot(data=data, x=\"x\", y=\"y\")\n\n\n\n\n\n\n\n\nWe can clearly visually see here that there is a linear relationship between the \\(x\\) and \\(y\\) values but we need to be able to extract the exact parameters programmatically.\n\n\nLinear regression\n\nSetting up our model\nFor this and for the other machine learning techniques in this course, we will be using scikit-learn. It provides a whole host of tools for studying data. You may also want to investigate statsmodels which also provides a large number of tools for statistical exploration.\nscikit-learn provides a number of models which you can use to study your data. Each model is a Python class which can be imported and used. The usual process for using a model is:\n\nImport the model you want to use\nCreate an instance of that model and set any hyperparameters you want\nFit the model to the data, this computes the parameters of the model using machine learning\nPredict new information using the model\n\nAs we saw by plotting the data, the relationship between \\(x\\) and \\(y\\) is linear. In scitkit-learn, linear regression is available as scikit-learn.linear_model.LinearRegression.\nWe import the model and create an instance of it. By default the LinearRegression model will fit the y-intercept, but since we don’t want to make that assumption we explicitly pass fit_intercept=True. fit_intercept is an example of a hyperparameter, which are variables or options in a model which you set up-front rather than letting them be learned from the data.\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=True)\nmodel\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniNot fittedLinearRegression() \n\n\nWhen running a Jupyter Notebook, you will see this model summary box appear every time a cell evaluates to the model itself. Here, writing model causes this to happen explicitly but it will happen any time the last line in a cell has the return value of the model too.\n\n\nFitting the data\nOnce we have created our model, we can fit it to the data by calling the fit() method on it. This takes two arguments:\n\nThe input data as a two-dimensional structure of the size \\((N_{samples}, N_{features})\\).\nThe labels or targets of the data as a one-dimensional data structure of size \\((N_{samples})\\).\n\nIn our case we only have one feature, \\(x\\), and 50 data points so it should be in the shape \\((50, 1)\\). If we just request data[\"x\"] then that will be a 1D array (actually a pandas Series) of shape \\((50)\\) so we must request the data with data[[\"x\"]] (which returns it as a single-column, but still two-dimensional, DataFrame). For a more thorough explanation of this dimensionality difference, see this explanation.\nIf you’re using pandas to store your data (as we are) then just remember that the first argument should be a DataFrame and the second should be a Series.\n\nX = data[[\"x\"]]\ny = data[\"y\"]\n\n\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\nIt is when you call this function that scikit-learn will go away and perform the machine learning algorithm. In our case it takes a fraction of a second but more complex models could take hours to compute.\nBy the time that this function returns, the model object will have had it’s internal parameters set as best the algorithm can do in order to predict \\(y\\) from \\(x\\).\nWe see the summary box appear here too as the fit method returns the model that it was called on, allowing you to chain together methods if you want.\n\n\nMaking predictions using the model\nOnce we’ve performed the fit, we can use it to predict the value of new data points which weren’t part of the original data set.\nWe can use this to plot the fit over the original data to compare the result. By getting the predicted \\(y\\) values for the minimum and maximum \\(x\\) values, we can plot a straight line between them to visualise the model.\nThe predict() function takes an array of the same shape as the original input data (\\((N_{samples}, N_{features})\\)) so we put our list of \\(x\\) values into a DataFrame before passing it to predict().\nWe then plot the original data in the same way as before and draw the prediction line in the same plot.\n\npred = pd.DataFrame({\"x\": [0, 10]})  # Make a new DataFrame containing the X values\npred[\"y\"] = model.predict(pred)  # Make a prediction and add that data into the table\npred\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n-4.903311\n\n\n1\n10\n14.873255\n\n\n\n\n\n\n\n\nimport seaborn as sns\nsns.relplot(data=data, x=\"x\", y=\"y\")\nsns.lineplot(data=pred, x=\"x\", y=\"y\", c=\"red\", linestyle=\":\")\n\n\n\n\n\n\n\n\nAs well as plotting the line in a graph, we can also extract the calculated values of the gradient and y-intercept. The gradient is available as a list of values, model.coef_, one for each dimension or feature. The intercept is available as model.intercept_:\n\nprint(\" Model gradient: \", model.coef_[0])\nprint(\"Model intercept:\", model.intercept_)\n\n Model gradient:  1.9776566003853107\nModel intercept: -4.9033107255311155\n\n\nThe equation that we have extracted can therefore be represented as:\n\\[y = 1.97 x - 4.90\\]\nThe original data was produced (with random wobble applied) from a straight line with gradient \\(2\\) and y-intercept of \\(-5\\). Our model has managed to predict values very close to the original.\n\n\n\n\n\n\nExercise\n\n\n\n\nRun the above data reading and model fitting. Ensure that you get the same answer we got above.\nTry fitting without allowing the y-intercept to vary. How does it affect the prediction of the gradient?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/linear.csv\")\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=False)\nX = data[[\"x\"]]\ny = data[\"y\"]\nmodel.fit(X, y)\n\n\n\nLinearRegression(fit_intercept=False)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression(fit_intercept=False)\n\n\n\n\n\npred = pd.DataFrame({\"x\": [0, 10]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=data, x=\"x\", y=\"y\")\nsns.lineplot(data=pred, x=\"x\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='x', ylabel='y'&gt;\n\nprint(\" Model gradient: \", model.coef_[0])\nprint(\"Model intercept:\", model.intercept_)\n Model gradient:  1.1985226874421444\nModel intercept: 0.0",
    "crumbs": [
      "Fitting data"
    ]
  }
]