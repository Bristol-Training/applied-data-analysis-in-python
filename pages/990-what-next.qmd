---
title: What next
---


## Further topics

Here are some additional chapters to work through on a number of different topics. Choose the ones that interest you.

[**Feature scaling and principal component analysis**](./appendix_scaling.qmd) are important parts of many data analysis pipelines.

[**Clustering**](appendix_clustering.qmd) is an unsupervised classification method.

[**Image Clustering**](appendix_clustering_images.qmd) uses clustering techniques to demonstrate a form of image compression.

## Other concepts

This course has provided a quick overview of some of the basic data analysis and machine learning tools available. Of course it could not cover the full breadth of possible topics so here I will give some pointers to things you may want to learn next.

**Naïve bayes classification** is a supervised classification method which works by attempting to a model by assuming that the data you present it with was created from that model originally. It uses Bayesian statistics to work out which model parameters best describe the distribution of data. Read more at [scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html).

**Support vector machines** is another supervised classification method which tries to find the dividing line between different classes. Read more at [scikit-learn](https://scikit-learn.org/stable/modules/svm.html).

**Decision trees** is a supervised classification method which creates a tree of binary choices in order to assign a class to a data point. For example, on a population data set, the first question might be "is the person's height above 1.6 m". Depending on the answer to that, the next question asked may be different. The path through the tree depends on the exact details of the data point and so each leaf will be associated with a predicted class. Due to the large number of potential parameter combinations, DTs require more data that many other methods but are capable of creating a more nuanced response. Read more at [scikit-learn](https://scikit-learn.org/stable/modules/tree.html).

**Neural networks** are the most widely know technique and are generally used as a classification tool for both supervised and unsupervised situations. They are very versatile and are often the first tool reached for by data scientists, even when there is a simpler method available.

**K-Folds cross-validation** is a more advanced technique for testing and validating your models. It greatly increases the time to fit your model but if you can afford it it is worth using. scikit-learn has [built-in suport for it](https://scikit-learn.org/stable/modules/cross_validation.html).

## Further reading

- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlas
- [scikit-learn documentation](https://scikit-learn.org/stable/documentation.html)
- [Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](http://shop.oreilly.com/product/0636920142874.do) by Aurélien Géron.

