{
  "hash": "c026390b473b1aea2691a79403d1289b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Correlation\n---\n\n\n\nWhen presented with a new collection of data, one of the first questions you may ask is how they are related to each other. This can involve deep study of how one parameter is likely to vary as you change another but the simplest start is to look a the linear correlation between them.\n\nCorrelation is usually taught as being the degree to which two variables are *linearly* related, that is as one increases, on average how much does the other one increase. This is a useful measure because it's easy to calculate and most data only have either linear relationships or no relationship at all.\n\n<img src=\"../img/linear.svg\" style=\"box-shadow: none;\" width=400>\n\nHowever, correlation is a much broader idea than that and when doing machine learning, it's worth understanding the bigger picture. At its core, correlation is a measure of how *related* two data sets are. The way I like to think of it is, if I know the value of one of the two ariables, how much information do I have about the value of the other.\n\nTo highlight this, consider the following two variables, $x$ and $y$:\n\n<img src=\"../img/quadratic.svg\" style=\"box-shadow: none;\" width=400>\n\nThey have a linear correlation of zero (*on average* as $x$ increases, $y$ stays the same) but if you know the value of $X$, you clearly have *information* about what the value of $y$ is likely to be.\n\nThe other way to think about it is in terms of *mutual information*. $y$ is clearly sharing information with $x$, otherwise there would be no visible pattern.\n\n## Multiple cross-correlation\n\nIt's very common when working on real data that you have more than two figures of interest.\n\nTo get a sense of some real data, let's look at a [housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) provided by scikit-learn.\n\n\n::: {#981824bc .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\n\nhousing, target = fetch_california_housing(as_frame=True, return_X_y=True)\n```\n:::\n\n\n::: {#739fe3da .cell execution_count=2}\n``` {.python .cell-code}\nhousing.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.3252</td>\n      <td>41.0</td>\n      <td>6.984127</td>\n      <td>1.023810</td>\n      <td>322.0</td>\n      <td>2.555556</td>\n      <td>37.88</td>\n      <td>-122.23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.3014</td>\n      <td>21.0</td>\n      <td>6.238137</td>\n      <td>0.971880</td>\n      <td>2401.0</td>\n      <td>2.109842</td>\n      <td>37.86</td>\n      <td>-122.22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.2574</td>\n      <td>52.0</td>\n      <td>8.288136</td>\n      <td>1.073446</td>\n      <td>496.0</td>\n      <td>2.802260</td>\n      <td>37.85</td>\n      <td>-122.24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.6431</td>\n      <td>52.0</td>\n      <td>5.817352</td>\n      <td>1.073059</td>\n      <td>558.0</td>\n      <td>2.547945</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.8462</td>\n      <td>52.0</td>\n      <td>6.281853</td>\n      <td>1.081081</td>\n      <td>565.0</td>\n      <td>2.181467</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIt has a row for each census block and a column for each feature, e.g. \"median income of the block\", \"average house age of the block\" etc.\n\nTo get the linear correlation between all these features, we call the `corr()` method on the `DataFrame`:\n\n::: {#9490f380 .cell execution_count=3}\n``` {.python .cell-code}\nhousing.corr()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MedInc</th>\n      <td>1.000000</td>\n      <td>-0.119034</td>\n      <td>0.326895</td>\n      <td>-0.062040</td>\n      <td>0.004834</td>\n      <td>0.018766</td>\n      <td>-0.079809</td>\n      <td>-0.015176</td>\n    </tr>\n    <tr>\n      <th>HouseAge</th>\n      <td>-0.119034</td>\n      <td>1.000000</td>\n      <td>-0.153277</td>\n      <td>-0.077747</td>\n      <td>-0.296244</td>\n      <td>0.013191</td>\n      <td>0.011173</td>\n      <td>-0.108197</td>\n    </tr>\n    <tr>\n      <th>AveRooms</th>\n      <td>0.326895</td>\n      <td>-0.153277</td>\n      <td>1.000000</td>\n      <td>0.847621</td>\n      <td>-0.072213</td>\n      <td>-0.004852</td>\n      <td>0.106389</td>\n      <td>-0.027540</td>\n    </tr>\n    <tr>\n      <th>AveBedrms</th>\n      <td>-0.062040</td>\n      <td>-0.077747</td>\n      <td>0.847621</td>\n      <td>1.000000</td>\n      <td>-0.066197</td>\n      <td>-0.006181</td>\n      <td>0.069721</td>\n      <td>0.013344</td>\n    </tr>\n    <tr>\n      <th>Population</th>\n      <td>0.004834</td>\n      <td>-0.296244</td>\n      <td>-0.072213</td>\n      <td>-0.066197</td>\n      <td>1.000000</td>\n      <td>0.069863</td>\n      <td>-0.108785</td>\n      <td>0.099773</td>\n    </tr>\n    <tr>\n      <th>AveOccup</th>\n      <td>0.018766</td>\n      <td>0.013191</td>\n      <td>-0.004852</td>\n      <td>-0.006181</td>\n      <td>0.069863</td>\n      <td>1.000000</td>\n      <td>0.002366</td>\n      <td>0.002476</td>\n    </tr>\n    <tr>\n      <th>Latitude</th>\n      <td>-0.079809</td>\n      <td>0.011173</td>\n      <td>0.106389</td>\n      <td>0.069721</td>\n      <td>-0.108785</td>\n      <td>0.002366</td>\n      <td>1.000000</td>\n      <td>-0.924664</td>\n    </tr>\n    <tr>\n      <th>Longitude</th>\n      <td>-0.015176</td>\n      <td>-0.108197</td>\n      <td>-0.027540</td>\n      <td>0.013344</td>\n      <td>0.099773</td>\n      <td>0.002476</td>\n      <td>-0.924664</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nHere we see the features in our data set along both the rows and the columns. The correlation between each pair is given as a number between -1.0 and 1.0 where -1.0 is absolute inverse linear correlation, 1.0 is absolute positive linear correlation and zero is no linear correlation.\n\nWe see the the 1.0 occuring on the diagonal (because a variable is always completely correlated with itself) and a whole range of values between -1.0 and 1.0 off-diagonal.\n\nIf we want the correlation between two specific columns then we can request it from this object:\n\n::: {#dce29077 .cell execution_count=4}\n``` {.python .cell-code}\ncorr = housing.corr()\ncorr[\"MedInc\"][\"AveRooms\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nnp.float64(0.3268954316412978)\n```\n:::\n:::\n\n\n::: {#exampleN .callout-note icon=false title='Exercise'}\nLook through the table manually and see if you can find the most negative and most positive correlations.\n\nBonus: Try to automate that search using Python code.\n  - Hint: To find the minimum, use the `min()` and `idxmin()` methods. To find the maximum, hide the diagonals first using `np.fill_diagonal(corr.values, np.nan)`\n:::\n\n::: {#answerN .callout-caution icon=false title='Answer' collapse=\"true\"}\n\nLet's find the most negative and the most positive (ignoring self-correlation) values\n\n\n```python\nfrom pandas import DataFrame\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhousing = DataFrame(housing_data.data, columns=housing_data.feature_names)\n\ncorr = housing.corr()\n\ncorr\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MedInc</th>\n      <td>1.000000</td>\n      <td>-0.119034</td>\n      <td>0.326895</td>\n      <td>-0.062040</td>\n      <td>0.004834</td>\n      <td>0.018766</td>\n      <td>-0.079809</td>\n      <td>-0.015176</td>\n    </tr>\n    <tr>\n      <th>HouseAge</th>\n      <td>-0.119034</td>\n      <td>1.000000</td>\n      <td>-0.153277</td>\n      <td>-0.077747</td>\n      <td>-0.296244</td>\n      <td>0.013191</td>\n      <td>0.011173</td>\n      <td>-0.108197</td>\n    </tr>\n    <tr>\n      <th>AveRooms</th>\n      <td>0.326895</td>\n      <td>-0.153277</td>\n      <td>1.000000</td>\n      <td>0.847621</td>\n      <td>-0.072213</td>\n      <td>-0.004852</td>\n      <td>0.106389</td>\n      <td>-0.027540</td>\n    </tr>\n    <tr>\n      <th>AveBedrms</th>\n      <td>-0.062040</td>\n      <td>-0.077747</td>\n      <td>0.847621</td>\n      <td>1.000000</td>\n      <td>-0.066197</td>\n      <td>-0.006181</td>\n      <td>0.069721</td>\n      <td>0.013344</td>\n    </tr>\n    <tr>\n      <th>Population</th>\n      <td>0.004834</td>\n      <td>-0.296244</td>\n      <td>-0.072213</td>\n      <td>-0.066197</td>\n      <td>1.000000</td>\n      <td>0.069863</td>\n      <td>-0.108785</td>\n      <td>0.099773</td>\n    </tr>\n    <tr>\n      <th>AveOccup</th>\n      <td>0.018766</td>\n      <td>0.013191</td>\n      <td>-0.004852</td>\n      <td>-0.006181</td>\n      <td>0.069863</td>\n      <td>1.000000</td>\n      <td>0.002366</td>\n      <td>0.002476</td>\n    </tr>\n    <tr>\n      <th>Latitude</th>\n      <td>-0.079809</td>\n      <td>0.011173</td>\n      <td>0.106389</td>\n      <td>0.069721</td>\n      <td>-0.108785</td>\n      <td>0.002366</td>\n      <td>1.000000</td>\n      <td>-0.924664</td>\n    </tr>\n    <tr>\n      <th>Longitude</th>\n      <td>-0.015176</td>\n      <td>-0.108197</td>\n      <td>-0.027540</td>\n      <td>0.013344</td>\n      <td>0.099773</td>\n      <td>0.002476</td>\n      <td>-0.924664</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n## Most negative correlation\n\nFind the most negative correlation for each column:\n\n\n```python\ncorr.min()\n```\n\n\n\n\n    MedInc       -0.119034\n    HouseAge     -0.296244\n    AveRooms     -0.153277\n    AveBedrms    -0.077747\n    Population   -0.296244\n    AveOccup     -0.006181\n    Latitude     -0.924664\n    Longitude    -0.924664\n    dtype: float64\n\n\n\nFind the column which has the lowest correlation:\n\n\n```python\ncorr.min().idxmin()\n```\n\n\n\n\n    'Latitude'\n\n\n\nExtract the Latitude column and get the index of the most negative value in it:\n\n\n```python\ncorr[corr.min().idxmin()].idxmin()\n```\n\n\n\n\n    'Longitude'\n\n\n\nThe most negative correlation is therefore between:\n\n\n```python\ncorr.min().idxmin(), corr[corr.min().idxmin()].idxmin()\n```\n\n\n\n\n    ('Latitude', 'Longitude')\n\n\n\nwith the value:\n\n\n```python\ncorr.min().min()\n```\n\n\n\n\n    -0.9246644339150366\n\n\n\n## Most positive correlation\n\nFirst we need to remove the 1.0 values on the diagonal:\n\n\n```python\nimport numpy as np\n\nnp.fill_diagonal(corr.values, np.nan)\ncorr\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MedInc</th>\n      <td>NaN</td>\n      <td>-0.119034</td>\n      <td>0.326895</td>\n      <td>-0.062040</td>\n      <td>0.004834</td>\n      <td>0.018766</td>\n      <td>-0.079809</td>\n      <td>-0.015176</td>\n    </tr>\n    <tr>\n      <th>HouseAge</th>\n      <td>-0.119034</td>\n      <td>NaN</td>\n      <td>-0.153277</td>\n      <td>-0.077747</td>\n      <td>-0.296244</td>\n      <td>0.013191</td>\n      <td>0.011173</td>\n      <td>-0.108197</td>\n    </tr>\n    <tr>\n      <th>AveRooms</th>\n      <td>0.326895</td>\n      <td>-0.153277</td>\n      <td>NaN</td>\n      <td>0.847621</td>\n      <td>-0.072213</td>\n      <td>-0.004852</td>\n      <td>0.106389</td>\n      <td>-0.027540</td>\n    </tr>\n    <tr>\n      <th>AveBedrms</th>\n      <td>-0.062040</td>\n      <td>-0.077747</td>\n      <td>0.847621</td>\n      <td>NaN</td>\n      <td>-0.066197</td>\n      <td>-0.006181</td>\n      <td>0.069721</td>\n      <td>0.013344</td>\n    </tr>\n    <tr>\n      <th>Population</th>\n      <td>0.004834</td>\n      <td>-0.296244</td>\n      <td>-0.072213</td>\n      <td>-0.066197</td>\n      <td>NaN</td>\n      <td>0.069863</td>\n      <td>-0.108785</td>\n      <td>0.099773</td>\n    </tr>\n    <tr>\n      <th>AveOccup</th>\n      <td>0.018766</td>\n      <td>0.013191</td>\n      <td>-0.004852</td>\n      <td>-0.006181</td>\n      <td>0.069863</td>\n      <td>NaN</td>\n      <td>0.002366</td>\n      <td>0.002476</td>\n    </tr>\n    <tr>\n      <th>Latitude</th>\n      <td>-0.079809</td>\n      <td>0.011173</td>\n      <td>0.106389</td>\n      <td>0.069721</td>\n      <td>-0.108785</td>\n      <td>0.002366</td>\n      <td>NaN</td>\n      <td>-0.924664</td>\n    </tr>\n    <tr>\n      <th>Longitude</th>\n      <td>-0.015176</td>\n      <td>-0.108197</td>\n      <td>-0.027540</td>\n      <td>0.013344</td>\n      <td>0.099773</td>\n      <td>0.002476</td>\n      <td>-0.924664</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ncorr.max().idxmax(), corr[corr.max().idxmax()].idxmax()\n```\n\n\n\n\n    ('AveRooms', 'AveBedrms')\n\n\n\n\n```python\ncorr.max().max()\n```\n\n\n\n\n    0.8476213257130424\n\n\n\n\n:::\n\n\n## Plotting the correlation\n\nViewing the correlation coefficients as a table is useful if you want the precise value of the correlation but often you want a visual overview which can give you the information you want at a glance.\n\nThe easiest way to view it is as a heat map where each cell has a colour showing the value of the correlation using Seaborn which is a visualisation library that provides a higher-level interface to Matplotlib.\n\n::: {#935bc131 .cell execution_count=5}\n``` {.python .cell-code}\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, square=True, cmap=\"RdBu\")\n```\n\n::: {.cell-output .cell-output-display}\n![](600-correlation_files/figure-html/cell-6-output-1.png){width=568 height=479}\n:::\n:::\n\n\nThis gives us a sense of which parameters are strongly correlated with each other. Very blue squares are positively correlated, for example the average number of rooms and the average number of bedrooms. That correlation makes sense as they definitely have mutual information.\n\nOthers perhaps make less sense at a glance. We see that the latitude is very strongly negatively correlated with the longitude. Why on earth should there be any relationship between those two? Let's take a look at another view on the data to see if we can discover why.\n\n## Multi-variable scatter matrix\n\nPandas also provides a quick method of looking at a large number of data parameters at once and looking visually at which might be worth investigating. If you pass any pandas `DataFrame` to the `scatter_matrix()` function then it will plot all the *pairs* of parameters in the data.\n\nThe produced graph has a lot of information in it so it's worth taking some time to make sure you understand these plots. The plot is arranged with all the variables of interest from top to bottom and then repeated from left to right so that any one square in the grid is defined by the intersection of two variables.\n\nEach box that is an intersection of a variable with another (e.g. row three, column one is the intersection between \"AveRooms\" and \"MedInc\") shows the scatter plot of how the values of those variables relate to each other. If you see a strong diagonal line it means that those variables are correlated in this data set. It it's more of a blob or a flat horizontal or vertical line then that suggests a low correlation.\n\nThe top-right triangle of the plot is a repeat of the bottom-left triangle, just with the items in the pair reversed (i.e. row one, column three is the intersection between \"MedInc\" and \"AveRooms\").\n\nThe square boxes along the diagonal from the top-left to the bottom-right are those intersections of a variable with itself and so are used, not to show correlation, but to show the distribution of values of each single variable as a histogram.\n\n::: {#f6f1fde8 .cell execution_count=6}\n``` {.python .cell-code}\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(housing, figsize=(16, 16))\n```\n\n::: {.cell-output .cell-output-display}\n![](600-correlation_files/figure-html/cell-7-output-1.png){width=1277 height=1264}\n:::\n:::\n\n\nIn general, when calculating a regression, you want your features to be as uncorrelated with each other as possible. This is because if two features, $x_1$ and $x_2$ are strongly correlated with each other then it's possible to predict the value of $x_2$ from the value of $x_1$ with high confidence. This means that $x_2$ is not providing any additional predictive power.\n\nIn some cases this is not a problem as adding one extra variable does not slow down or harm the algorithm used but some methods benefit from choosing carefully the parameters which are being fitted over.\n\nIt's also possible in some cases to transform the data in some way to reduce the correlation between variables. One example of a method which does this is principle component analysis (PCA).\n\nOn the other hand, you *do* want correlation between $X$ and $y$ as if there is no mutual information then there is no predictive power.\n\n\n::: {#exampleN .callout-note icon=false title='Exercise'}\n\nTry running through the above step using a different dataset from sklearn. You can find them listed at https://scikit-learn.org/stable/datasets/toy_dataset.html. _Iris_ is a classic dataset used in machine learning which it is worth being aware of.\n\n:::\n\n::: {#answerN .callout-caution icon=false title='Answer' collapse=\"true\"}\n\n```python\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\n\niris, iris_target = load_iris(as_frame=True, return_X_y=True)\niris.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ncorr = iris.corr()\n```\n\n\n```python\n%matplotlib inline\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, cmap=\"RdBu\")\n```\n\n\n\n\n    <AxesSubplot: >\n\n\n\n\n    \n![](../img/answer_iris_correlation_2_1.png)\n    \n\n\n\n```python\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(iris, figsize=(16, 16), c=iris_target)\n```\n\n\n    \n![](../img/answer_iris_correlation_3_0.png)\n    \n\n\n\n:::\n\n",
    "supporting": [
      "600-correlation_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}